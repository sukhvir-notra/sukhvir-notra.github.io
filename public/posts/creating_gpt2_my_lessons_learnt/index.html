<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>üí° Creating GPT2: My lessons learnt | Sukhvir's blog</title>
<meta name=keywords content="AI"><meta name=description content="My journey building GPT2, following Andrej Karpathy's tutorials. My personal insights, challenges, and lessons learned - from data prep to distributed training. A candid look at creating a large language model and practical tips."><meta name=author content="Sukhvir Notra"><link rel=canonical href=http://localhost:1313/posts/creating_gpt2_my_lessons_learnt/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon.png><link rel=apple-touch-icon href=http://localhost:1313/favicon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/creating_gpt2_my_lessons_learnt/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/creating_gpt2_my_lessons_learnt/"><meta property="og:site_name" content="Sukhvir's blog"><meta property="og:title" content="üí° Creating GPT2: My lessons learnt"><meta property="og:description" content="My journey building GPT2, following Andrej Karpathy's tutorials. My personal insights, challenges, and lessons learned - from data prep to distributed training. A candid look at creating a large language model and practical tips."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-30T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-30T00:00:00+00:00"><meta property="article:tag" content="AI"><meta property="og:image" content="http://localhost:1313/images/GPT2_my_lessons_learnt/results-1.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/GPT2_my_lessons_learnt/results-1.png"><meta name=twitter:title content="üí° Creating GPT2: My lessons learnt"><meta name=twitter:description content="My journey building GPT2, following Andrej Karpathy's tutorials. My personal insights, challenges, and lessons learned - from data prep to distributed training. A candid look at creating a large language model and practical tips."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"üí° Creating GPT2: My lessons learnt","item":"http://localhost:1313/posts/creating_gpt2_my_lessons_learnt/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"üí° Creating GPT2: My lessons learnt","name":"üí° Creating GPT2: My lessons learnt","description":"My journey building GPT2, following Andrej Karpathy's tutorials. My personal insights, challenges, and lessons learned - from data prep to distributed training. A candid look at creating a large language model and practical tips.","keywords":["AI"],"articleBody":"Are you interested in creating an AI that can generate sonnets, tell jokes, or even help with your homework? In this blog, I‚Äôll guide you through my experience of building GPT-2, following Andrej Karpathy‚Äôs comprehensive ‚ÄúLet‚Äôs build GPT‚Äù tutorial series.\nIn this article, I‚Äôll share my notes and lessons learned as I delved into the intricacies of creating a large language model. I followed along with Karpathy‚Äôs videos (check them out here) and boy, did I learn a lot!\nFor those of you who want to peek under the hood, my code (which is essentially Karpathy‚Äôs with my modifications and a ton of comments for self-reference) is available on my GitHub. Don‚Äôt worry, I promise this journey will be more fun than watching paint dry (though, let‚Äôs be honest, sometimes that can be oddly satisfying). So, without further ado, let‚Äôs dive into the world of tokens, self-attention, and the occasional AI existential crisis. Grab your favourite caffeinated beverage, and let‚Äôs get started!\nTLDR:\nThis blog chronicles the journey of building and optimising a 124-million parameter GPT-2 model. Key steps included implementing self-attention mechanisms, adding multiple attention blocks with pre-normalization, and optimizing for computational efficiency using techniques like torch.compile and flash attention. The model was trained on a powerful GPU cluster, achieving competitive validation loss and surpassing OpenAI‚Äôs GPT-2 in accuracy on the HellaSwag benchmark. Despite not reaching GPT-3‚Äôs performance, the results highlight the effectiveness of targeted optimisations in deep learning.\nUnderstanding Self-Attention in Transformer Models Self-attention is a core concept in transformer models that enables each token in a sequence to interact with other tokens, capturing dependencies and relationships within the sequence. This interaction is governed by three key components for each token: Query, Key, and Value.\nQuery represents what a token is looking for in other tokens. Key represents what information a token has that might be of interest to other tokens. Value is the actual content that will be used for the computation. To begin, let‚Äôs set up the necessary parameters and define linear layers that will project the input tokens into queries, keys, and values. The code below defines linear layers that project the input tokens into queries, keys, and values. Each of these components is essential for the self-attention mechanism. Next, I worked with an example input tensor to see how the tokens are transformed into queries, keys, and values.\nSingle attention head\nTo compute the attention weights, I used the dot product of queries and keys (line 25). This calculation determines how much focus each token should pay to other tokens by taking the dot product between q (queries) and k (keys). The result is scaled to keep values in a manageable range.\nSince I‚Äôm dealing with a decoder block, I applied a mask to ensure that each token only attends to the tokens before it in the sequence (line 29). This masking is critical in preventing a token from ‚Äúseeing‚Äù future tokens, which would disrupt the sequence generation.\nFinally, I normalized the attention weights using softmax. This process of normalizing the attention scores and multiplying them with the values completes the self-attention mechanism.\nOne key observation is that self-attention does not inherently understand the order of tokens. This lack of spatial awareness means that transformers typically incorporate positional encoding to provide this information.\nIt‚Äôs also worth noting the difference between self-attention and cross-attention:\nSelf-attention: The queries, keys, and values all come from the same source. Cross-attention: The query comes from one source, while the keys and values are derived from another source. Lessons Learned: Query-Key-Value Mechanism: Understanding the distinct roles of queries, keys, and values is crucial for grasping how attention works in transformers. Masked Attention: Implementing masking in the decoder block is essential to prevent information leakage from future tokens. No Inherent Spatial Awareness: Self-attention does not inherently understand token order; positional encodings are necessary to introduce this information. Self vs. Cross-Attention: Recognizing the distinction between self-attention and cross-attention clarifies their roles within the transformer architecture. Expanding the Transformer with Attention Blocks and Pre-Normalisation Following the tutorial, the next step involved adding multiple blocks of attention heads and feedforward layers. As these layers were stacked, the neural network deepened significantly, which introduced potential optimization challenges. To mitigate these issues, the tutorial introduced the Add and Norm technique, which is crucial for maintaining stable training in deep networks.\nA notable deviation from the original ‚ÄúAttention is All You Need‚Äù paper was the use of pre-normalization instead of the typical post-normalization. This approach was taken to stabilize the training process as the network depth increased.\nTo prevent overfitting, a dropout layer was added. Here‚Äôs a snippet of my code that integrates these elements:\nFeedforward Network (MLP) with Dropout\nKarpathy emphasizes pre-normalization (applying LayerNorm before the linear layers) and includes a dropout layer to prevent overfitting.\nTransformer Block with Pre-Normalization and Dropout\nThe tutorial focuses solely on self-attention and feedforward blocks, without including cross-attention blocks. This is because the task at hand‚Äîtext prediction‚Äîrequires only past context, making cross-attention and encoders unnecessary. Instead, the implementation used a decoder-only architecture, employing a triangular mask to ensure each token only attends to previous tokens in the sequence.\nKey architectural choices highlighted in the tutorial:\nNo Cross-Attention: Focus solely on self-attention. Pre-Normalization: Layer normalization is applied before the attention and feedforward layers. Dropout: Added to prevent overfitting. Decoder-Only Architecture: The model is designed to focus on past context using triangular masking, which is well-suited for text prediction tasks. The results obtained from this implementation were:\nTrain Loss:** 1.1325 Validation Loss:** 1.1887\nHere is some sample generated text (based on the Harry Potter books corpus data set):\nhe dripped her face to get anyone else each other. ‚ÄòTharge I suppose behind it talks to find and the prefect Fleur, who want for anyone, Dumbledore‚Äôs sincent, of the marble on ghostly was wait to explain him for a restretching pain, black that the pair was squart. He wondered a continue to that he had not this attacked his like for that the memor. Harry saw Alofty Luna Jords to corridor, who had told But the parchment the window, her angless stretchy awimpage had done before he could carrier tha\nThe generated text demonstrates that the model learned to produce somewhat coherent sequences, though it still includes some nonsensical phrases, which is common at this stage of training.\nLessons Learned: Deep Network Challenges: Adding multiple attention and feedforward blocks can lead to optimization issues, making techniques like Add and Norm essential. Pre-Normalization: The use of pre-normalization, as recommended in the tutorial, proved helpful in stabilizing training in deeper models. Task-Specific Design: For text prediction, a decoder-only architecture with self-attention suffices, avoiding the complexity of cross-attention and encoders. Regularization: Incorporating dropout effectively prevents overfitting, which is crucial in a deep network with many parameters. Supersize Me: Scaling Up to 124 Million Parameters But wait, there‚Äôs more! Once you‚Äôve got your basic model up and running, it‚Äôs time to supersize it. The next phase was to create a 124-million parameter GPT-2 model.\nAt initialization, it‚Äôs expected that all vocabulary elements have a uniform probability of being the next character. Given the GPT-2 vocabulary size of 50,257, this means the initial probability for each character is 1/50257.\nGiven that the loss function is cross-entropy (or -log loss), the expected loss at initialization should be approximately:\nTraining such a large model efficiently requires thoughtful strategies. One key approach is the weight sharing scheme, which significantly reduces the number of parameters:\nCreating efficiencies: Weight sharing scheme\nThis weight sharing not only saves a significant amount of memory but also improves computational efficiency. It ensures that the model doesn‚Äôt need to maintain separate sets of weights for embedding and output, which is particularly advantageous in large-scale models like GPT-2.\nFor training, I used lambdalabs.com to set up a cluster with 8 A100 GPUs, each with 80GB of memory. This setup allowed for efficient training of the large model, which would be nearly impossible on a standard local machine.\nGPU cluster\nAnother useful trick for interacting with the code during runtime was using: import code; code.interact(local=locals()). This allowed me to pause the execution and interact with the current state of the code, which was invaluable for debugging and tweaking the model on the fly.\nExperimenting with different types of precisions, I found that using bf16 precision drastically improved performance. The time per iteration (dt) dropped from 4000ms on a local MacBook to approximately 96ms on the Lambda Labs cluster, making training much more efficient.\nLessons Learned: Expected Initial Loss: Understanding that the initial loss for a GPT-2 model is around 10.82 helps set realistic expectations at the start of training. Weight Sharing: Implementing weight sharing is a critical technique for reducing the parameter count and improving model efficiency. Efficient Hardware Use: Leveraging powerful GPUs, such as the A100s on Lambda Labs, is essential for training large models. Precision Matters: Switching to bf16 precision significantly reduces computation time, making large-scale model training more feasible. The Need for Speed: Optimizing Your AI To push the performance further, several optimizations were implemented. First up was torch.compile(), which brought the iteration time (dt) down to approximately 60ms.\nThe efficiency gain here comes from torch.compile‚Äôs ability to reduce multiple round trips between High Bandwidth Memory (HBM) and GPU cores. By streamlining calculations within the GPU cores and minimizing the data transfers back to HBM, significant time savings were achieved.\nHowever, torch.compile was just the beginning. Flash attention proved to be even more effective, especially for handling softmax operations. Flash attention fuses all attention operations within a transformer into a single, highly efficient kernel: F.scaled_dot_product_attention(q,k,v, is_causal = True)\nAnother optimization involved using non-ugly numbers‚Äîspecifically, adjusting the vocabulary size from 50,257 to 50,304, a number more amenable to power-of-2 operations. This adjustment slightly increases the tensor size, padding it with extra characters, but the resulting softmax probabilities for these padded characters are effectively ignored during computations. Despite the additional characters, this tweak boosts overall efficiency.\nThese optimizations collectively improved performance by 32x.\nFurther algorithmic improvements were based on insights from the GPT-3 paper:\nAdamW Optimizer: Betas were set to 0.9 and 0.95, with an epsilon of 1e-8. Gradient Clipping: Gradients were clipped to a norm of 1.0 to prevent large updates from bad batches. Learning Rate Scheduler: Implemented cosine decay with a warmup period. Weight Decay: Applied only to weight tensors, not biases, leveraging kernel fusion. Gradient Accumulation: Simulated a large batch size (up to 0.5 million) through gradient accumulation. To fully utilize the available hardware, Distributed Data Parallel (DDP) was introduced, spreading the workload across 8 GPUs. The training script was executed using:\ntorchrun --standalone --nproc_per_node=8 train_gpt2.py ... [other args] Within this setup, gradient synchronization was carefully managed to ensure efficiency:\nThis ensures that gradients are only synchronized during the final accumulation step, reducing overhead. It‚Äôs worth noting that this feature might be deprecated in the future, so ongoing monitoring is advised.\nAs the model scaled, so did the training dataset. The Hugging Face FineWeb-edu dataset (sample-10BT subset) was chosen for its high educational content, providing a substantial training corpus.\nFor evaluation, several strategies were implemented:\nEvaluation Frequency: An evaluation and sample generation were triggered every 100th step. Evaluation Dataset: The Hellswag dataset was used for this purpose. Learning Rate Experimentation: Unlike the standard approach, a higher learning rate was tested to observe its impact on the model‚Äôs learning. Lessons Learned Torch.compile() and Flash Attention: These optimizations are key for reducing computation time and enhancing performance. Power-of-2 Adjustments: Aligning tensor sizes to power-of-2 values can improve computational efficiency. Algorithmic Tweaks: Adopting strategies from the GPT-3 paper, such as specific optimizer settings and gradient clipping, significantly stabilizes training. Distributed Training: Utilizing multiple GPUs effectively with DDP is essential for scaling large models. Dataset Expansion: Growing the dataset and incorporating high-quality content is critical as the model size increases. Custom Evaluation Strategies: Regular evaluations and testing different learning rates provide valuable insights into model performance. Conclusion After implementing the various optimizations discussed, including advanced techniques like torch.compile, flash attention, and the use of non-ugly numbers, the 124-million parameter GPT-2 model showed significant improvements in both training efficiency and performance. As seen in the training and validation loss graph, the model reached a validation loss comparable to OpenAI‚Äôs GPT-2 implementation, indicating that the optimizations were effective in maintaining model accuracy while improving computational efficiency. Notably, our model achieved a validation loss of approximately 3.0, which aligns closely with OpenAI‚Äôs GPT-2 checkpoint.\nThe HellaSwag evaluation benchmark further highlighted the strengths of our optimized GPT-2 model. While it does not yet match the performance of OpenAI‚Äôs GPT-3 model, our implementation consistently outperformed the original GPT-2 baseline in terms of accuracy, steadily climbing to nearly 30%. This demonstrates that with targeted optimizations and careful attention to both hardware and algorithmic efficiency, it is possible to build and train large-scale models that approach the performance of industry-leading implementations. These results reinforce the importance of continuous experimentation and adaptation when working with deep learning models.\n","wordCount":"2161","inLanguage":"en","image":"http://localhost:1313/images/GPT2_my_lessons_learnt/results-1.png","datePublished":"2024-09-30T00:00:00Z","dateModified":"2024-09-30T00:00:00Z","author":{"@type":"Person","name":"Sukhvir Notra"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/creating_gpt2_my_lessons_learnt/"},"publisher":{"@type":"Organization","name":"Sukhvir's blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Sukhvir's blog (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/archives/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/categories/ title=Categories><span>Categories</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">üí° Creating GPT2: My lessons learnt</h1><div class=post-description>My journey building GPT2, following Andrej Karpathy's tutorials. My personal insights, challenges, and lessons learned - from data prep to distributed training. A candid look at creating a large language model and practical tips.</div><div class=post-meta><span title='2024-09-30 00:00:00 +0000 UTC'>September 30, 2024</span>&nbsp;¬∑&nbsp;11 min&nbsp;¬∑&nbsp;2161 words&nbsp;¬∑&nbsp;Sukhvir Notra</div></header><figure class=entry-cover><img loading=eager src=http://localhost:1313/images/GPT2_my_lessons_learnt/results-1.png alt="Training and validation loss graph"><figcaption>Training and validation loss graph</figcaption></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#understanding-self-attention-in-transformer-models>Understanding Self-Attention in Transformer Models</a><ul><li><a href=#lessons-learned>Lessons Learned:</a></li></ul></li><li><a href=#expanding-the-transformer-with-attention-blocks-and-pre-normalisation>Expanding the Transformer with Attention Blocks and Pre-Normalisation</a><ul><li><a href=#lessons-learned-1>Lessons Learned:</a></li></ul></li><li><a href=#supersize-me-scaling-up-to-124-million-parameters>Supersize Me: Scaling Up to 124 Million Parameters</a><ul><li><a href=#lessons-learned-2>Lessons Learned:</a></li></ul></li><li><a href=#the-need-for-speed-optimizing-your-ai>The Need for Speed: Optimizing Your AI</a></li><li><a href=#lessons-learned-3>Lessons Learned</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><p>Are you interested in creating an AI that can generate sonnets, tell jokes, or even help with your homework? In this blog, I‚Äôll guide you through my experience of building GPT-2, following Andrej Karpathy‚Äôs comprehensive &ldquo;Let&rsquo;s build GPT&rdquo; tutorial series.</p><p>In this article, I&rsquo;ll share my notes and lessons learned as I delved into the intricacies of creating a large language model. I followed along with Karpathy&rsquo;s videos (check them out <a href="https://www.youtube.com/watch?v=l8pRSuU81PU&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&amp;index=10&amp;ref=sukhvir-ai.ghost.io">here</a>) and boy, did I learn a lot!</p><p>For those of you who want to peek under the hood, my code (which is essentially Karpathy&rsquo;s with my modifications and a ton of comments for self-reference) is available on <a href="https://github.com/sukhvir-notra/gpt2?ref=sukhvir-ai.ghost.io">my GitHub</a>. Don&rsquo;t worry, I promise this journey will be more fun than watching paint dry (though, let&rsquo;s be honest, sometimes that can be oddly satisfying).
So, without further ado, let&rsquo;s dive into the world of tokens, self-attention, and the occasional AI existential crisis. Grab your favourite caffeinated beverage, and let&rsquo;s get started!</p><hr><p><strong>TLDR</strong>:</p><blockquote><p>This blog chronicles the journey of building and optimising a 124-million parameter GPT-2 model. Key steps included implementing self-attention mechanisms, adding multiple attention blocks with pre-normalization, and optimizing for computational efficiency using techniques like torch.compile and flash attention. The model was trained on a powerful GPU cluster, achieving competitive validation loss and surpassing OpenAI‚Äôs GPT-2 in accuracy on the HellaSwag benchmark. Despite not reaching GPT-3‚Äôs performance, the results highlight the effectiveness of targeted optimisations in deep learning.</p></blockquote><hr><h2 id=understanding-self-attention-in-transformer-models>Understanding Self-Attention in Transformer Models<a hidden class=anchor aria-hidden=true href=#understanding-self-attention-in-transformer-models>#</a></h2><p>Self-attention is a core concept in transformer models that enables each token in a sequence to interact with other tokens, capturing dependencies and relationships within the sequence. This interaction is governed by three key components for each token: <strong>Query</strong>, <strong>Key</strong>, and <strong>Value</strong>.</p><ul><li><strong>Query</strong> represents what a token is looking for in other tokens.</li><li><strong>Key</strong> represents what information a token has that might be of interest to other tokens.</li><li><strong>Value</strong> is the actual content that will be used for the computation.</li></ul><p>To begin, let&rsquo;s set up the necessary parameters and define linear layers that will project the input tokens into queries, keys, and values. The code below defines linear layers that project the input tokens into queries, keys, and values. Each of these components is essential for the self-attention mechanism. Next, I worked with an example input tensor to see how the tokens are transformed into queries, keys, and values.</p><figure class=align-center><img loading=lazy src=/images/GPT2_my_lessons_learnt/single_attention_head.png#center alt="Single attention head"><figcaption><p>Single attention head</p></figcaption></figure><p>To compute the attention weights, I used the <em>dot product</em> of queries and keys (line 25). This calculation determines how much focus each token should pay to other tokens by taking the dot product between <code>q</code> (<em>queries</em>) and <code>k</code> (<em>keys</em>). The result is scaled to keep values in a manageable range.</p><p>Since I&rsquo;m dealing with a decoder block, I applied a <em>mask</em> to ensure that each token only attends to the tokens before it in the sequence (line 29). This masking is critical in preventing a token from &ldquo;seeing&rdquo; future tokens, which would disrupt the sequence generation.</p><p>Finally, I normalized the attention weights using <em>softmax</em>. This process of normalizing the attention scores and multiplying them with the values completes the self-attention mechanism.</p><p>One key observation is that self-attention does not inherently understand the order of tokens. This lack of spatial awareness means that transformers typically incorporate positional encoding to provide this information.</p><p>It&rsquo;s also worth noting the difference between self-attention and cross-attention:</p><ul><li><strong>Self-attention:</strong> The queries, keys, and values all come from the same source.</li><li><strong>Cross-attention:</strong> The query comes from one source, while the keys and values are derived from another source.</li></ul><h3 id=lessons-learned>Lessons Learned:<a hidden class=anchor aria-hidden=true href=#lessons-learned>#</a></h3><ul><li><strong>Query-Key-Value Mechanism:</strong> Understanding the distinct roles of queries, keys, and values is crucial for grasping how attention works in transformers.</li><li><strong>Masked Attention:</strong> Implementing masking in the decoder block is essential to prevent information leakage from future tokens.</li><li><strong>No Inherent Spatial Awareness:</strong> Self-attention does not inherently understand token order; positional encodings are necessary to introduce this information.</li><li><strong>Self vs. Cross-Attention:</strong> Recognizing the distinction between self-attention and cross-attention clarifies their roles within the transformer architecture.</li></ul><h2 id=expanding-the-transformer-with-attention-blocks-and-pre-normalisation>Expanding the Transformer with Attention Blocks and Pre-Normalisation<a hidden class=anchor aria-hidden=true href=#expanding-the-transformer-with-attention-blocks-and-pre-normalisation>#</a></h2><p>Following the tutorial, the next step involved adding multiple blocks of attention heads and feedforward layers. As these layers were stacked, the neural network deepened significantly, which introduced potential optimization challenges. To mitigate these issues, the tutorial introduced the <em><strong>Add and Norm</strong></em> technique, which is crucial for maintaining stable training in deep networks.</p><p>A notable deviation from the original &ldquo;Attention is All You Need&rdquo; paper was the use of <em><strong>pre-normalization</strong></em> instead of the typical post-normalization. This approach was taken to stabilize the training process as the network depth increased.</p><p>To prevent overfitting, a dropout layer was added. Here‚Äôs a snippet of my code that integrates these elements:</p><figure class=align-center><img loading=lazy src=/images/GPT2_my_lessons_learnt/feedforward.png#center alt="Feedforward Network (MLP) with Dropout"><figcaption><p>Feedforward Network (MLP) with Dropout</p></figcaption></figure><p>Karpathy emphasizes pre-normalization (applying <code>LayerNorm</code> before the linear layers) and includes a dropout layer to prevent overfitting.</p><figure class=align-center><img loading=lazy src=/images/GPT2_my_lessons_learnt/transformer_block.png#center alt="Transformer Block with Pre-Normalization and Dropout"><figcaption><p>Transformer Block with Pre-Normalization and Dropout</p></figcaption></figure><p>The tutorial focuses solely on <strong>self-attention</strong> and feedforward blocks, without including cross-attention blocks. This is because the task at hand‚Äîtext prediction‚Äîrequires only past context, making cross-attention and encoders unnecessary. Instead, the implementation used a decoder-only architecture, employing a triangular mask to ensure each token only attends to previous tokens in the sequence.</p><p>Key architectural choices highlighted in the tutorial:</p><ul><li><strong>No Cross-Attention:</strong> Focus solely on self-attention.</li><li><strong>Pre-Normalization:</strong> Layer normalization is applied before the attention and feedforward layers.</li><li><strong>Dropout:</strong> Added to prevent overfitting.</li><li><strong>Decoder-Only Architecture:</strong> The model is designed to focus on past context using triangular masking, which is well-suited for text prediction tasks.</li></ul><p>The results obtained from this implementation were:</p><p><code>Train Loss:** 1.1325</code>
<code>Validation Loss:** 1.1887</code></p><p>Here is some sample generated text (based on the Harry Potter books corpus data set):</p><blockquote><p>he dripped her face to get anyone else each other. &lsquo;Tharge I suppose behind it talks to find and the prefect Fleur, who want for anyone, Dumbledore&rsquo;s sincent, of the marble on ghostly was wait to explain him for a restretching pain, black that the pair was squart. He wondered a continue to that he had not this attacked his like for that the memor. Harry saw Alofty Luna Jords to corridor, who had told But the parchment the window, her angless stretchy awimpage had done before he could carrier tha</p></blockquote><p>The generated text demonstrates that the model learned to produce somewhat coherent sequences, though it still includes some nonsensical phrases, which is common at this stage of training.</p><h3 id=lessons-learned-1>Lessons Learned:<a hidden class=anchor aria-hidden=true href=#lessons-learned-1>#</a></h3><ul><li><strong>Deep Network Challenges:</strong> Adding multiple attention and feedforward blocks can lead to optimization issues, making techniques like <em>Add and Norm</em> essential.</li><li><strong>Pre-Normalization:</strong> The use of pre-normalization, as recommended in the tutorial, proved helpful in stabilizing training in deeper models.</li><li><strong>Task-Specific Design:</strong> For text prediction, a decoder-only architecture with self-attention suffices, avoiding the complexity of cross-attention and encoders.</li><li><strong>Regularization:</strong> Incorporating dropout effectively prevents overfitting, which is crucial in a deep network with many parameters.</li></ul><h2 id=supersize-me-scaling-up-to-124-million-parameters>Supersize Me: Scaling Up to 124 Million Parameters<a hidden class=anchor aria-hidden=true href=#supersize-me-scaling-up-to-124-million-parameters>#</a></h2><p>But wait, there&rsquo;s more! Once you&rsquo;ve got your basic model up and running, it&rsquo;s time to supersize it. The next phase was to create a 124-million parameter GPT-2 model.</p><p>At initialization, it&rsquo;s expected that all vocabulary elements have a uniform probability of being the next character. Given the GPT-2 vocabulary size of <code>50,257</code>, this means the initial probability for each character is <code>1/50257</code>.</p><p>Given that the loss function is cross-entropy (or -log loss), the expected loss at initialization should be approximately:</p><figure class=align-center><img loading=lazy src=/images/GPT2_my_lessons_learnt/loss.png#center></figure><p>Training such a large model efficiently requires thoughtful strategies. One key approach is the <strong>weight sharing scheme</strong>, which significantly reduces the number of parameters:</p><figure class=align-center><img loading=lazy src=/images/GPT2_my_lessons_learnt/weight_sharing.png#center alt="Creating efficiencies: Weight sharing scheme"><figcaption><p>Creating efficiencies: Weight sharing scheme</p></figcaption></figure><p>This weight sharing not only saves a significant amount of memory but also improves computational efficiency. It ensures that the model doesn&rsquo;t need to maintain separate sets of weights for embedding and output, which is particularly advantageous in large-scale models like GPT-2.</p><p>For training, I used <a href="https://cloud.lambdalabs.com/instances?ref=sukhvir-ai.ghost.io">lambdalabs.com</a> to set up a cluster with 8 A100 GPUs, each with 80GB of memory. This setup allowed for efficient training of the large model, which would be nearly impossible on a standard local machine.</p><figure class=align-center><img loading=lazy src=/images/GPT2_my_lessons_learnt/gpu.png#center alt="GPU cluster"><figcaption><p>GPU cluster</p></figcaption></figure><p>Another useful trick for interacting with the code during runtime was using: <code>import code; code.interact(local=locals())</code>. This allowed me to pause the execution and interact with the current state of the code, which was invaluable for debugging and tweaking the model on the fly.</p><p>Experimenting with different types of precisions, I found that using <strong>bf16</strong> precision drastically improved performance. The time per iteration (<code>dt</code>) dropped from <em><strong>4000ms</strong></em> on a local MacBook to approximately <em>96ms</em> on the Lambda Labs cluster, making training much more efficient.</p><h3 id=lessons-learned-2>Lessons Learned:<a hidden class=anchor aria-hidden=true href=#lessons-learned-2>#</a></h3><ul><li><strong>Expected Initial Loss:</strong> Understanding that the initial loss for a GPT-2 model is around <strong>10.82</strong> helps set realistic expectations at the start of training.</li><li><strong>Weight Sharing:</strong> Implementing weight sharing is a critical technique for reducing the parameter count and improving model efficiency.</li><li><strong>Efficient Hardware Use:</strong> Leveraging powerful GPUs, such as the A100s on Lambda Labs, is essential for training large models.</li><li><strong>Precision Matters:</strong> Switching to <strong>bf16</strong> precision significantly reduces computation time, making large-scale model training more feasible.</li></ul><h2 id=the-need-for-speed-optimizing-your-ai>The Need for Speed: Optimizing Your AI<a hidden class=anchor aria-hidden=true href=#the-need-for-speed-optimizing-your-ai>#</a></h2><p>To push the performance further, several optimizations were implemented. First up was <code>torch.compile()</code>, which brought the iteration time (<code>dt</code>) down to approximately <strong>60ms</strong>.</p><p>The efficiency gain here comes from <code>torch.compile</code>&rsquo;s ability to reduce multiple round trips between High Bandwidth Memory (HBM) and GPU cores. By streamlining calculations within the GPU cores and minimizing the data transfers back to HBM, significant time savings were achieved.</p><p>However, <code>torch.compile</code> was just the beginning. <em><strong>Flash attention</strong></em> proved to be even more effective, especially for handling softmax operations. Flash attention fuses all attention operations within a transformer into a single, highly efficient kernel:
<code>F.scaled_dot_product_attention(q,k,v, is_causal = True)</code></p><p>Another optimization involved using <strong>non-ugly numbers</strong>‚Äîspecifically, adjusting the vocabulary size from <code>50,257</code> to <code>50,304</code>, a number more amenable to power-of-2 operations. This adjustment slightly increases the tensor size, padding it with extra characters, but the resulting softmax probabilities for these padded characters are effectively ignored during computations. Despite the additional characters, this tweak boosts overall efficiency.</p><p>These optimizations collectively improved performance by <strong>32x</strong>.</p><p>Further algorithmic improvements were based on insights from the GPT-3 paper:</p><ul><li><strong>AdamW Optimizer:</strong> Betas were set to 0.9 and 0.95, with an epsilon of 1e-8.</li><li><strong>Gradient Clipping:</strong> Gradients were clipped to a norm of 1.0 to prevent large updates from bad batches.</li><li><strong>Learning Rate Scheduler:</strong> Implemented cosine decay with a warmup period.</li><li><strong>Weight Decay:</strong> Applied only to weight tensors, not biases, leveraging kernel fusion.</li><li><strong>Gradient Accumulation:</strong> Simulated a large batch size (up to 0.5 million) through gradient accumulation.</li></ul><p>To fully utilize the available hardware, <strong>Distributed Data Parallel (DDP)</strong> was introduced, spreading the workload across 8 GPUs. The training script was executed using:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>torchrun --standalone --nproc_per_node<span class=o>=</span><span class=m>8</span> train_gpt2.py ... <span class=o>[</span>other args<span class=o>]</span>
</span></span></code></pre></div><p>Within this setup, gradient synchronization was carefully managed to ensure efficiency:</p><figure class=align-center><img loading=lazy src=/images/GPT2_my_lessons_learnt/grad-sync.png#center></figure><p>This ensures that gradients are only synchronized during the final accumulation step, reducing overhead. It&rsquo;s worth noting that this feature might be deprecated in the future, so ongoing monitoring is advised.</p><p>As the model scaled, so did the training dataset. The Hugging Face <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu?ref=sukhvir-ai.ghost.io">FineWeb-edu dataset</a> (sample-10BT subset) was chosen for its high educational content, providing a substantial training corpus.</p><p>For evaluation, several strategies were implemented:</p><ul><li><strong>Evaluation Frequency:</strong> An evaluation and sample generation were triggered every 100th step.</li><li><strong>Evaluation Dataset:</strong> The Hellswag dataset was used for this purpose.</li><li><strong>Learning Rate Experimentation:</strong> Unlike the standard approach, a higher learning rate was tested to observe its impact on the model&rsquo;s learning.</li></ul><h2 id=lessons-learned-3>Lessons Learned<a hidden class=anchor aria-hidden=true href=#lessons-learned-3>#</a></h2><ul><li><strong>Torch.compile() and Flash Attention:</strong> These optimizations are key for reducing computation time and enhancing performance.</li><li><strong>Power-of-2 Adjustments:</strong> Aligning tensor sizes to power-of-2 values can improve computational efficiency.</li><li><strong>Algorithmic Tweaks:</strong> Adopting strategies from the GPT-3 paper, such as specific optimizer settings and gradient clipping, significantly stabilizes training.</li><li><strong>Distributed Training:</strong> Utilizing multiple GPUs effectively with DDP is essential for scaling large models.</li><li><strong>Dataset Expansion:</strong> Growing the dataset and incorporating high-quality content is critical as the model size increases.</li><li><strong>Custom Evaluation Strategies:</strong> Regular evaluations and testing different learning rates provide valuable insights into model performance.</li></ul><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>After implementing the various optimizations discussed, including advanced techniques like <code>torch.compile</code>, flash attention, and the use of non-ugly numbers, the 124-million parameter GPT-2 model showed significant improvements in both training efficiency and performance. As seen in the training and validation loss graph, the model reached a validation loss comparable to OpenAI&rsquo;s GPT-2 implementation, indicating that the optimizations were effective in maintaining model accuracy while improving computational efficiency. Notably, our model achieved a validation loss of approximately 3.0, which aligns closely with OpenAI‚Äôs GPT-2 checkpoint.</p><figure class=align-center><img loading=lazy src=/images/GPT2_my_lessons_learnt/results-1.png#center></figure><p>The HellaSwag evaluation benchmark further highlighted the strengths of our optimized GPT-2 model. While it does not yet match the performance of OpenAI‚Äôs GPT-3 model, our implementation consistently outperformed the original GPT-2 baseline in terms of accuracy, steadily climbing to nearly 30%. This demonstrates that with targeted optimizations and careful attention to both hardware and algorithmic efficiency, it is possible to build and train large-scale models that approach the performance of industry-leading implementations. These results reinforce the importance of continuous experimentation and adaptation when working with deep learning models.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/ai/>AI</a></li></ul></footer><script src=https://giscus.app/client.js data-repo=sukhvir-notra/sukhvir-notra.github.io data-repo-id=R_kgDOOU5LDg data-category=General data-category-id=DIC_kwDOOU5LDs4Co1dN data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Sukhvir's blog</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>