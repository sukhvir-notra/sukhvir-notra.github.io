<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Sukhvir's blog</title>
<meta name=keywords content><meta name=description content="There&rsquo;s been a lot of talk around cyber security of artificial intelligence and there are many many many examples and papers online where people are discussing the the cyber security implications of generative AI and large language models. A lot of the examples that I&rsquo;m seeing online are things what I would call social engineering of a large language model, where there are a bunch of different examples of how we can trick the large language models to bypass the safety guardrails and respond to us in a particular manner such that it bypasses a guardrail. So show me how to create a ransomware. Most large language models of today will sort of deny that request saying this goes against their content policies or ethical obligations or safety guidelines that they have been put in place from the reinforced human learning and feedback RLHF or RHLF sort of life cycles and pipelines during the development and sort of creation of these large language models. But a lot of the examples online sort of talk about prompt engineering and sort of prompt injection as a way of getting the large language models to bypass those safety and guardrails and sort of reveal the malicious answer. This is great. This is good to see. But this is, I don&rsquo;t know where the cybersecurity implication here is. Yes, it&rsquo;s a risk in the sense that the large language models cannot be relied upon to maintain their safety guidelines, unintended way by subjecting it to certain precisely constructed prompts to get it to do that. I would like some examples where how this will actually apply in an organization that can be leveraged into a full-on attack scenario. How can this actually work? Because this seems like social engineering of a large language model to get it to do something based on some of the ways you are trying to influence it to do it. Are there any attacks on a large language model itself akin to an attack on a software library or an application that you download from the internet which might have inherent vulnerability that the attackers might exploit? Are there any cybersecurity attacks or papers out there or proven demonstrations that do that against any open source or foundational large language models because that would be very very interesting where i can see sort of prompt engineering coming into the foray is if i imagine a scenario like where we have uh security teams are relying upon a large language model to monitor telemetry of an organization and when there is a anomaly in the telemetry when something which means there&rsquo;s something going on in the organization that is unpredictable, unpredicted or unforeseen or something that is out of the ordinary. Perhaps the large language model has been enabled to execute certain binaries to go and prevent or sort of prevent or remediate whatever is going on in the organization. So what we could do as an attacker would be to sort of create malicious activity within the organization, knowing full well that it will be detected, but the goal is to be detected, and sort of plant or replace the binary or a script that the large language model uses to deal with that kind of problem. So let&rsquo;s say you attempt to create, you download a malicious binary onto a Windows device in your organization. This will certainly trigger Windows Defender alerts to, for example, Sentinel. And let&rsquo;s say you have a large language model that&rsquo;s monitoring Sentinel alerts, and it has a script that goes ahead and quarantines or deletes the malicious binary from a Windows device, for example. So as an attacker, you planted this binary into the Windows device with the full intention of it being detected and an alert being generated. At the same time, you replace that deletion binary that the LLM uses to clean up with your own malicious script. But you do not have the permissions to launch that script, or if you do have permissions, you don&rsquo;t have enough sufficient permissions like the admin. You&rsquo;re not privileged escalated to execute with the right amount of permissions. So by forcing a binary onto a Windows device, you&rsquo;ve triggered an alert. The large language model sort of looks at the alert. Large language model then triggers, as part of its automated processes, that deletion binary and deletion binary is corrupted in a malicious binary that you planted, and thereby sort of using a large language model as a way of privilege escalating and running a particular script of your design but this is no different to typical attacks llms in this particular case are just like any other applications that are being used to to sort of execute other files so you&rsquo;re using one using a privileged process to launch another process, thereby making sure that the secondary process that has been launched has the privileges of the first process. But are there any other examples of using large language models or exploiting large language models that go beyond prompt engineering of those large language models. If so, please feel free to comment and let me know about it."><meta name=author content="Sukhvir Notra"><link rel=canonical href=http://localhost:1313/posts/cyber_attacks_on_llms/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon.png><link rel=apple-touch-icon href=http://localhost:1313/favicon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/cyber_attacks_on_llms/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/cyber_attacks_on_llms/"><meta property="og:site_name" content="Sukhvir's blog"><meta property="og:title" content="Sukhvir's blog"><meta property="og:description" content="There’s been a lot of talk around cyber security of artificial intelligence and there are many many many examples and papers online where people are discussing the the cyber security implications of generative AI and large language models. A lot of the examples that I’m seeing online are things what I would call social engineering of a large language model, where there are a bunch of different examples of how we can trick the large language models to bypass the safety guardrails and respond to us in a particular manner such that it bypasses a guardrail. So show me how to create a ransomware. Most large language models of today will sort of deny that request saying this goes against their content policies or ethical obligations or safety guidelines that they have been put in place from the reinforced human learning and feedback RLHF or RHLF sort of life cycles and pipelines during the development and sort of creation of these large language models. But a lot of the examples online sort of talk about prompt engineering and sort of prompt injection as a way of getting the large language models to bypass those safety and guardrails and sort of reveal the malicious answer. This is great. This is good to see. But this is, I don’t know where the cybersecurity implication here is. Yes, it’s a risk in the sense that the large language models cannot be relied upon to maintain their safety guidelines, unintended way by subjecting it to certain precisely constructed prompts to get it to do that. I would like some examples where how this will actually apply in an organization that can be leveraged into a full-on attack scenario. How can this actually work? Because this seems like social engineering of a large language model to get it to do something based on some of the ways you are trying to influence it to do it. Are there any attacks on a large language model itself akin to an attack on a software library or an application that you download from the internet which might have inherent vulnerability that the attackers might exploit? Are there any cybersecurity attacks or papers out there or proven demonstrations that do that against any open source or foundational large language models because that would be very very interesting where i can see sort of prompt engineering coming into the foray is if i imagine a scenario like where we have uh security teams are relying upon a large language model to monitor telemetry of an organization and when there is a anomaly in the telemetry when something which means there’s something going on in the organization that is unpredictable, unpredicted or unforeseen or something that is out of the ordinary. Perhaps the large language model has been enabled to execute certain binaries to go and prevent or sort of prevent or remediate whatever is going on in the organization. So what we could do as an attacker would be to sort of create malicious activity within the organization, knowing full well that it will be detected, but the goal is to be detected, and sort of plant or replace the binary or a script that the large language model uses to deal with that kind of problem. So let’s say you attempt to create, you download a malicious binary onto a Windows device in your organization. This will certainly trigger Windows Defender alerts to, for example, Sentinel. And let’s say you have a large language model that’s monitoring Sentinel alerts, and it has a script that goes ahead and quarantines or deletes the malicious binary from a Windows device, for example. So as an attacker, you planted this binary into the Windows device with the full intention of it being detected and an alert being generated. At the same time, you replace that deletion binary that the LLM uses to clean up with your own malicious script. But you do not have the permissions to launch that script, or if you do have permissions, you don’t have enough sufficient permissions like the admin. You’re not privileged escalated to execute with the right amount of permissions. So by forcing a binary onto a Windows device, you’ve triggered an alert. The large language model sort of looks at the alert. Large language model then triggers, as part of its automated processes, that deletion binary and deletion binary is corrupted in a malicious binary that you planted, and thereby sort of using a large language model as a way of privilege escalating and running a particular script of your design but this is no different to typical attacks llms in this particular case are just like any other applications that are being used to to sort of execute other files so you’re using one using a privileged process to launch another process, thereby making sure that the secondary process that has been launched has the privileges of the first process. But are there any other examples of using large language models or exploiting large language models that go beyond prompt engineering of those large language models. If so, please feel free to comment and let me know about it."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="There&rsquo;s been a lot of talk around cyber security of artificial intelligence and there are many many many examples and papers online where people are discussing the the cyber security implications of generative AI and large language models. A lot of the examples that I&rsquo;m seeing online are things what I would call social engineering of a large language model, where there are a bunch of different examples of how we can trick the large language models to bypass the safety guardrails and respond to us in a particular manner such that it bypasses a guardrail. So show me how to create a ransomware. Most large language models of today will sort of deny that request saying this goes against their content policies or ethical obligations or safety guidelines that they have been put in place from the reinforced human learning and feedback RLHF or RHLF sort of life cycles and pipelines during the development and sort of creation of these large language models. But a lot of the examples online sort of talk about prompt engineering and sort of prompt injection as a way of getting the large language models to bypass those safety and guardrails and sort of reveal the malicious answer. This is great. This is good to see. But this is, I don&rsquo;t know where the cybersecurity implication here is. Yes, it&rsquo;s a risk in the sense that the large language models cannot be relied upon to maintain their safety guidelines, unintended way by subjecting it to certain precisely constructed prompts to get it to do that. I would like some examples where how this will actually apply in an organization that can be leveraged into a full-on attack scenario. How can this actually work? Because this seems like social engineering of a large language model to get it to do something based on some of the ways you are trying to influence it to do it. Are there any attacks on a large language model itself akin to an attack on a software library or an application that you download from the internet which might have inherent vulnerability that the attackers might exploit? Are there any cybersecurity attacks or papers out there or proven demonstrations that do that against any open source or foundational large language models because that would be very very interesting where i can see sort of prompt engineering coming into the foray is if i imagine a scenario like where we have uh security teams are relying upon a large language model to monitor telemetry of an organization and when there is a anomaly in the telemetry when something which means there&rsquo;s something going on in the organization that is unpredictable, unpredicted or unforeseen or something that is out of the ordinary. Perhaps the large language model has been enabled to execute certain binaries to go and prevent or sort of prevent or remediate whatever is going on in the organization. So what we could do as an attacker would be to sort of create malicious activity within the organization, knowing full well that it will be detected, but the goal is to be detected, and sort of plant or replace the binary or a script that the large language model uses to deal with that kind of problem. So let&rsquo;s say you attempt to create, you download a malicious binary onto a Windows device in your organization. This will certainly trigger Windows Defender alerts to, for example, Sentinel. And let&rsquo;s say you have a large language model that&rsquo;s monitoring Sentinel alerts, and it has a script that goes ahead and quarantines or deletes the malicious binary from a Windows device, for example. So as an attacker, you planted this binary into the Windows device with the full intention of it being detected and an alert being generated. At the same time, you replace that deletion binary that the LLM uses to clean up with your own malicious script. But you do not have the permissions to launch that script, or if you do have permissions, you don&rsquo;t have enough sufficient permissions like the admin. You&rsquo;re not privileged escalated to execute with the right amount of permissions. So by forcing a binary onto a Windows device, you&rsquo;ve triggered an alert. The large language model sort of looks at the alert. Large language model then triggers, as part of its automated processes, that deletion binary and deletion binary is corrupted in a malicious binary that you planted, and thereby sort of using a large language model as a way of privilege escalating and running a particular script of your design but this is no different to typical attacks llms in this particular case are just like any other applications that are being used to to sort of execute other files so you&rsquo;re using one using a privileged process to launch another process, thereby making sure that the secondary process that has been launched has the privileges of the first process. But are there any other examples of using large language models or exploiting large language models that go beyond prompt engineering of those large language models. If so, please feel free to comment and let me know about it."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"","item":"http://localhost:1313/posts/cyber_attacks_on_llms/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"","name":"","description":"There\u0026rsquo;s been a lot of talk around cyber security of artificial intelligence and there are many many many examples and papers online where people are discussing the the cyber security implications of generative AI and large language models. A lot of the examples that I\u0026rsquo;m seeing online are things what I would call social engineering of a large language model, where there are a bunch of different examples of how we can trick the large language models to bypass the safety guardrails and respond to us in a particular manner such that it bypasses a guardrail. So show me how to create a ransomware. Most large language models of today will sort of deny that request saying this goes against their content policies or ethical obligations or safety guidelines that they have been put in place from the reinforced human learning and feedback RLHF or RHLF sort of life cycles and pipelines during the development and sort of creation of these large language models. But a lot of the examples online sort of talk about prompt engineering and sort of prompt injection as a way of getting the large language models to bypass those safety and guardrails and sort of reveal the malicious answer. This is great. This is good to see. But this is, I don\u0026rsquo;t know where the cybersecurity implication here is. Yes, it\u0026rsquo;s a risk in the sense that the large language models cannot be relied upon to maintain their safety guidelines, unintended way by subjecting it to certain precisely constructed prompts to get it to do that. I would like some examples where how this will actually apply in an organization that can be leveraged into a full-on attack scenario. How can this actually work? Because this seems like social engineering of a large language model to get it to do something based on some of the ways you are trying to influence it to do it. Are there any attacks on a large language model itself akin to an attack on a software library or an application that you download from the internet which might have inherent vulnerability that the attackers might exploit? Are there any cybersecurity attacks or papers out there or proven demonstrations that do that against any open source or foundational large language models because that would be very very interesting where i can see sort of prompt engineering coming into the foray is if i imagine a scenario like where we have uh security teams are relying upon a large language model to monitor telemetry of an organization and when there is a anomaly in the telemetry when something which means there\u0026rsquo;s something going on in the organization that is unpredictable, unpredicted or unforeseen or something that is out of the ordinary. Perhaps the large language model has been enabled to execute certain binaries to go and prevent or sort of prevent or remediate whatever is going on in the organization. So what we could do as an attacker would be to sort of create malicious activity within the organization, knowing full well that it will be detected, but the goal is to be detected, and sort of plant or replace the binary or a script that the large language model uses to deal with that kind of problem. So let\u0026rsquo;s say you attempt to create, you download a malicious binary onto a Windows device in your organization. This will certainly trigger Windows Defender alerts to, for example, Sentinel. And let\u0026rsquo;s say you have a large language model that\u0026rsquo;s monitoring Sentinel alerts, and it has a script that goes ahead and quarantines or deletes the malicious binary from a Windows device, for example. So as an attacker, you planted this binary into the Windows device with the full intention of it being detected and an alert being generated. At the same time, you replace that deletion binary that the LLM uses to clean up with your own malicious script. But you do not have the permissions to launch that script, or if you do have permissions, you don\u0026rsquo;t have enough sufficient permissions like the admin. You\u0026rsquo;re not privileged escalated to execute with the right amount of permissions. So by forcing a binary onto a Windows device, you\u0026rsquo;ve triggered an alert. The large language model sort of looks at the alert. Large language model then triggers, as part of its automated processes, that deletion binary and deletion binary is corrupted in a malicious binary that you planted, and thereby sort of using a large language model as a way of privilege escalating and running a particular script of your design but this is no different to typical attacks llms in this particular case are just like any other applications that are being used to to sort of execute other files so you\u0026rsquo;re using one using a privileged process to launch another process, thereby making sure that the secondary process that has been launched has the privileges of the first process. But are there any other examples of using large language models or exploiting large language models that go beyond prompt engineering of those large language models. If so, please feel free to comment and let me know about it.\n","keywords":[],"articleBody":"There’s been a lot of talk around cyber security of artificial intelligence and there are many many many examples and papers online where people are discussing the the cyber security implications of generative AI and large language models. A lot of the examples that I’m seeing online are things what I would call social engineering of a large language model, where there are a bunch of different examples of how we can trick the large language models to bypass the safety guardrails and respond to us in a particular manner such that it bypasses a guardrail. So show me how to create a ransomware. Most large language models of today will sort of deny that request saying this goes against their content policies or ethical obligations or safety guidelines that they have been put in place from the reinforced human learning and feedback RLHF or RHLF sort of life cycles and pipelines during the development and sort of creation of these large language models. But a lot of the examples online sort of talk about prompt engineering and sort of prompt injection as a way of getting the large language models to bypass those safety and guardrails and sort of reveal the malicious answer. This is great. This is good to see. But this is, I don’t know where the cybersecurity implication here is. Yes, it’s a risk in the sense that the large language models cannot be relied upon to maintain their safety guidelines, unintended way by subjecting it to certain precisely constructed prompts to get it to do that. I would like some examples where how this will actually apply in an organization that can be leveraged into a full-on attack scenario. How can this actually work? Because this seems like social engineering of a large language model to get it to do something based on some of the ways you are trying to influence it to do it. Are there any attacks on a large language model itself akin to an attack on a software library or an application that you download from the internet which might have inherent vulnerability that the attackers might exploit? Are there any cybersecurity attacks or papers out there or proven demonstrations that do that against any open source or foundational large language models because that would be very very interesting where i can see sort of prompt engineering coming into the foray is if i imagine a scenario like where we have uh security teams are relying upon a large language model to monitor telemetry of an organization and when there is a anomaly in the telemetry when something which means there’s something going on in the organization that is unpredictable, unpredicted or unforeseen or something that is out of the ordinary. Perhaps the large language model has been enabled to execute certain binaries to go and prevent or sort of prevent or remediate whatever is going on in the organization. So what we could do as an attacker would be to sort of create malicious activity within the organization, knowing full well that it will be detected, but the goal is to be detected, and sort of plant or replace the binary or a script that the large language model uses to deal with that kind of problem. So let’s say you attempt to create, you download a malicious binary onto a Windows device in your organization. This will certainly trigger Windows Defender alerts to, for example, Sentinel. And let’s say you have a large language model that’s monitoring Sentinel alerts, and it has a script that goes ahead and quarantines or deletes the malicious binary from a Windows device, for example. So as an attacker, you planted this binary into the Windows device with the full intention of it being detected and an alert being generated. At the same time, you replace that deletion binary that the LLM uses to clean up with your own malicious script. But you do not have the permissions to launch that script, or if you do have permissions, you don’t have enough sufficient permissions like the admin. You’re not privileged escalated to execute with the right amount of permissions. So by forcing a binary onto a Windows device, you’ve triggered an alert. The large language model sort of looks at the alert. Large language model then triggers, as part of its automated processes, that deletion binary and deletion binary is corrupted in a malicious binary that you planted, and thereby sort of using a large language model as a way of privilege escalating and running a particular script of your design but this is no different to typical attacks llms in this particular case are just like any other applications that are being used to to sort of execute other files so you’re using one using a privileged process to launch another process, thereby making sure that the secondary process that has been launched has the privileges of the first process. But are there any other examples of using large language models or exploiting large language models that go beyond prompt engineering of those large language models. If so, please feel free to comment and let me know about it.\n","wordCount":"867","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Sukhvir Notra"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/cyber_attacks_on_llms/"},"publisher":{"@type":"Organization","name":"Sukhvir's blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Sukhvir's blog (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/archives/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/categories/ title=Categories><span>Categories</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent"></h1><div class=post-meta>5 min&nbsp;·&nbsp;867 words&nbsp;·&nbsp;Sukhvir Notra</div></header><div class=post-content><p>There&rsquo;s been a lot of talk around cyber security of artificial intelligence and there are many many many examples and papers online where people are discussing the the cyber security implications of generative AI and large language models. A lot of the examples that I&rsquo;m seeing online are things what I would call social engineering of a large language model, where there are a bunch of different examples of how we can trick the large language models to bypass the safety guardrails and respond to us in a particular manner such that it bypasses a guardrail. So show me how to create a ransomware. Most large language models of today will sort of deny that request saying this goes against their content policies or ethical obligations or safety guidelines that they have been put in place from the reinforced human learning and feedback RLHF or RHLF sort of life cycles and pipelines during the development and sort of creation of these large language models. But a lot of the examples online sort of talk about prompt engineering and sort of prompt injection as a way of getting the large language models to bypass those safety and guardrails and sort of reveal the malicious answer. This is great. This is good to see. But this is, I don&rsquo;t know where the cybersecurity implication here is. Yes, it&rsquo;s a risk in the sense that the large language models cannot be relied upon to maintain their safety guidelines, unintended way by subjecting it to certain precisely constructed prompts to get it to do that. I would like some examples where how this will actually apply in an organization that can be leveraged into a full-on attack scenario. How can this actually work? Because this seems like social engineering of a large language model to get it to do something based on some of the ways you are trying to influence it to do it. Are there any attacks on a large language model itself akin to an attack on a software library or an application that you download from the internet which might have inherent vulnerability that the attackers might exploit? Are there any cybersecurity attacks or papers out there or proven demonstrations that do that against any open source or foundational large language models because that would be very very interesting where i can see sort of prompt engineering coming into the foray is if i imagine a scenario like where we have uh security teams are relying upon a large language model to monitor telemetry of an organization and when there is a anomaly in the telemetry when something which means there&rsquo;s something going on in the organization that is unpredictable, unpredicted or unforeseen or something that is out of the ordinary. Perhaps the large language model has been enabled to execute certain binaries to go and prevent or sort of prevent or remediate whatever is going on in the organization. So what we could do as an attacker would be to sort of create malicious activity within the organization, knowing full well that it will be detected, but the goal is to be detected, and sort of plant or replace the binary or a script that the large language model uses to deal with that kind of problem. So let&rsquo;s say you attempt to create, you download a malicious binary onto a Windows device in your organization. This will certainly trigger Windows Defender alerts to, for example, Sentinel. And let&rsquo;s say you have a large language model that&rsquo;s monitoring Sentinel alerts, and it has a script that goes ahead and quarantines or deletes the malicious binary from a Windows device, for example. So as an attacker, you planted this binary into the Windows device with the full intention of it being detected and an alert being generated. At the same time, you replace that deletion binary that the LLM uses to clean up with your own malicious script. But you do not have the permissions to launch that script, or if you do have permissions, you don&rsquo;t have enough sufficient permissions like the admin. You&rsquo;re not privileged escalated to execute with the right amount of permissions. So by forcing a binary onto a Windows device, you&rsquo;ve triggered an alert. The large language model sort of looks at the alert. Large language model then triggers, as part of its automated processes, that deletion binary and deletion binary is corrupted in a malicious binary that you planted, and thereby sort of using a large language model as a way of privilege escalating and running a particular script of your design but this is no different to typical attacks llms in this particular case are just like any other applications that are being used to to sort of execute other files so you&rsquo;re using one using a privileged process to launch another process, thereby making sure that the secondary process that has been launched has the privileges of the first process. But are there any other examples of using large language models or exploiting large language models that go beyond prompt engineering of those large language models. If so, please feel free to comment and let me know about it.</p></div><footer class=post-footer><ul class=post-tags></ul></footer><script src=https://giscus.app/client.js data-repo=sukhvir-notra/sukhvir-notra.github.io data-repo-id=R_kgDOOU5LDg data-category=General data-category-id=DIC_kwDOOU5LDs4Co1dN data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Sukhvir's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>