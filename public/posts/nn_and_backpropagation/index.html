<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>üß† Neural Networks and Backpropagation | Sukhvir's blog</title>
<meta name=keywords content="AI"><meta name=description content="Follow my journey along Andrej Karpathy's YouTube series on AI/ML zero to hero training"><meta name=author content="Sukhvir Notra"><link rel=canonical href=http://localhost:1313/posts/nn_and_backpropagation/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon.png><link rel=apple-touch-icon href=http://localhost:1313/favicon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/nn_and_backpropagation/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/nn_and_backpropagation/"><meta property="og:site_name" content="Sukhvir's blog"><meta property="og:title" content="üß† Neural Networks and Backpropagation"><meta property="og:description" content="Follow my journey along Andrej Karpathy's YouTube series on AI/ML zero to hero training"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-01T00:00:00+00:00"><meta property="article:tag" content="AI"><meta property="og:image" content="http://localhost:1313/images/nn_and_backpropagation/cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/nn_and_backpropagation/cover.jpg"><meta name=twitter:title content="üß† Neural Networks and Backpropagation"><meta name=twitter:description content="Follow my journey along Andrej Karpathy's YouTube series on AI/ML zero to hero training"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"üß† Neural Networks and Backpropagation","item":"http://localhost:1313/posts/nn_and_backpropagation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"üß† Neural Networks and Backpropagation","name":"üß† Neural Networks and Backpropagation","description":"Follow my journey along Andrej Karpathy's YouTube series on AI/ML zero to hero training","keywords":["AI"],"articleBody":" Neural Networks \u0026 Backpropagation: My First Steps with Karpathy‚Äôs Zero to Hero Welcome to the first installment of my blog series inspired by Andrej Karpathy‚Äôs ‚ÄúZero to Hero‚Äù lecture series on AI and Machine Learning. Buckle up, because I‚Äôm diving headfirst into the magical world of neural networks and backpropagation!\nMy Jupyter notebook for this post: here\nA Calculus Refresher: Derivatives, Slopes, and Other Fun Stuff Karpathy kicks off the lecture with a crash course in basic calculus, focusing specifically on derivatives. Now, if you‚Äôre like me and calculus brings back memories of late-night study sessions and too much caffeine, fear not! This foundational knowledge is essential for understanding how neural networks (NNs) function.\nEssentially, I‚Äôm trying to wrap my head around how changing weights and biases‚Äîthe key parameters of NNs‚Äîaffect the output. In other words, derivatives help me figure out how much each weight in the network impacts the final output.\nKey takeaways:\nDerivatives and Impact: The derivative of the output with respect to each leaf node in the network shows me how much influence those nodes have on the overall output. This is crucial for tuning the NN effectively. Backpropagation Visualized: Karpathy walks through a detailed example of backpropagation using a pseudo neural network. This step-by-step process helps me visualize how backpropagation works and how chain rule derivatives come into play. Activation Functions Galore: I also touched on activation functions like tanh and sigmoid, which are the magic sauce that helps NNs make decisions. Automating Backpropagation: The lecture discusses how to algorithmically automate backpropagation and the importance of topological sort to ensure that I don‚Äôt backpropagate until the forward pass is complete. Level of Detail: Depending on my masochistic tendencies (or dedication to understanding every little detail), I can either implement the tanh function directly or break it down into its individual components. PyTorch to the Rescue: PyTorch offers some handy abstractions to make life easier. For instance, its tensors are float32 by default, but I can cast them to .double() for float64 precision, matching Python‚Äôs default floating-point precision. Also, I need to enable gradient calculation explicitly for leaf nodes (requires_grad=True), as they don‚Äôt do it by default for efficiency reasons. Here‚Äôs a snippet showing this in action:\nimport torch ''' Casting the tensors as doubles and enabling gradient calculation ''' x1 = torch.Tensor([2.0]).double() ; x1.requires_grad = True x2 = torch.Tensor([0.0]).double() ; x2.requires_grad = True w1 = torch.Tensor([-3.0]).double() ; w1.requires_grad = True w2 = torch.Tensor([1.0]).double() ; w2.requires_grad = True b = torch.Tensor([6.8813735870195432]).double() ; b.requires_grad = True # Forward pass calculation n = x1*w1 + x2*w2 + b o = torch.tanh(n) print(f\"Output (o): {o.data.item()}\") # Backward pass (calculating gradients) o.backward() print('--- Gradients ---') print(f'x2 grad: {x2.grad.item()}') print(f'w2 grad: {w2.grad.item()}') print(f'x1 grad: {x1.grad.item()}') print(f'w1 grad: {w1.grad.item()}') print(f'b grad: {b.grad.item()}') # Added bias gradient display Building Blocks: Neurons, Layers, and MLPs I then dove into the class definitions of a single neuron, a layer of neurons, and a Multi-Layer Perceptron (MLP). This section is all about understanding the fundamental building blocks of NNs and how they interconnect and work together.\nMain takeaways: Understanding the components is one thing; training them is another!\nThe Learning Rate Dilemma: One of the critical aspects of training NNs is choosing the appropriate learning rate. It‚Äôs a bit of a Goldilocks problem: too large a step size and I might overshoot the optimal minimum loss; too small and training will be painfully slow (or get stuck in local minima). Pro Tip - Zeroing the Grads: A key step before backpropagation in each training iteration in PyTorch is to zero out the gradients from the previous step (p.grad = 0.0). If I forget this, PyTorch accumulates gradients across iterations, leading to incorrect updates and a lot of head-scratching. Here‚Äôs a basic training loop structure illustrating this:\n# Assuming 'n' is our MLP model, 'xs' are inputs, 'ys' are target outputs # And we have defined a loss function (e.g., Mean Squared Error implicitly below) learning_rate = 0.05 epochs = 20 for k in range(epochs): # Forward pass: Get predictions for all inputs ypred = [n(x) for x in xs] # Calculate loss (sum of squared errors example) loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred)) # Backward pass # \u003e\u003e\u003e Crucial Step: Zero the gradients before calculating new ones \u003c\u003c\u003c for p in n.parameters(): p.grad = 0.0 loss.backward() # Calculate gradients for this batch # Update weights and biases with torch.no_grad(): # Temporarily disable gradient tracking for updates for p in n.parameters(): p.data += -learning_rate * p.grad # Gradient descent step print(f\"Epoch {k}, Loss: {loss.item()}\") # Use .item() to get Python number Summing It Up In summary, this first dive covered how neural networks are structured, how to calculate their loss (how wrong they are), and how to use backpropagation (leveraging the power of derivatives via the chain rule, implemented as gradient descent) to minimize this loss and make the network learn. I explored the tanh activation function, built the fundamental components of a neural network from scratch, and got my hands dirty with PyTorch basics.\nStay tuned for more posts in this series as I continue my journey from zero to hero in the world of AI and ML! And remember, in the immortal words of Karpathy (and perhaps a few stressed-out students), ‚ÄúHappy learning, and may your gradients always descend!‚Äù\n","wordCount":"888","inLanguage":"en","image":"http://localhost:1313/images/nn_and_backpropagation/cover.jpg","datePublished":"2024-09-01T00:00:00Z","dateModified":"2024-09-01T00:00:00Z","author":{"@type":"Person","name":"Sukhvir Notra"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/nn_and_backpropagation/"},"publisher":{"@type":"Organization","name":"Sukhvir's blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Sukhvir's blog (Alt + H)">Sukhvir's blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/archives/ title=Archives><span>Archives</span></a></li><li><a href=http://localhost:1313/categories/ title=Categories><span>Categories</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">üß† Neural Networks and Backpropagation</h1><div class=post-description>Follow my journey along Andrej Karpathy's YouTube series on AI/ML zero to hero training</div><div class=post-meta><span title='2024-09-01 00:00:00 +0000 UTC'>September 1, 2024</span>&nbsp;¬∑&nbsp;5 min&nbsp;¬∑&nbsp;888 words&nbsp;¬∑&nbsp;Sukhvir Notra</div></header><figure class=entry-cover><img loading=eager src=http://localhost:1313/images/nn_and_backpropagation/cover.jpg alt="Source: https://serokell.io/blog/understanding-backpropagation"><figcaption>Source: <a href=https://serokell.io/blog/understanding-backpropagation>https://serokell.io/blog/understanding-backpropagation</a></figcaption></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#a-calculus-refresher-derivatives-slopes-and-other-fun-stuff>A Calculus Refresher: Derivatives, Slopes, and Other Fun Stuff</a></li><li><a href=#building-blocks-neurons-layers-and-mlps>Building Blocks: Neurons, Layers, and MLPs</a><ul><li><a href=#main-takeaways>Main takeaways:</a></li></ul></li><li><a href=#summing-it-up>Summing It Up</a></li></ul></nav></div></details></div><div class=post-content><hr><h1 id=neural-networks--backpropagation-my-first-steps-with-karpathys-zero-to-hero>Neural Networks & Backpropagation: My First Steps with Karpathy&rsquo;s Zero to Hero<a hidden class=anchor aria-hidden=true href=#neural-networks--backpropagation-my-first-steps-with-karpathys-zero-to-hero>#</a></h1><p>Welcome to the first installment of my blog series inspired by Andrej Karpathy&rsquo;s &ldquo;Zero to Hero&rdquo; lecture series on AI and Machine Learning. Buckle up, because I&rsquo;m diving headfirst into the magical world of neural networks and backpropagation!</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/VMj-3S1tku0?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="YouTube video"></iframe></div><p><strong>My Jupyter notebook for this post:</strong> <a href="https://github.com/sukhvir-notra/AI-Learning-Zero-to-Hero?ref=sukhvir-ai.ghost.io">here</a></p><hr><h2 id=a-calculus-refresher-derivatives-slopes-and-other-fun-stuff>A Calculus Refresher: Derivatives, Slopes, and Other Fun Stuff<a hidden class=anchor aria-hidden=true href=#a-calculus-refresher-derivatives-slopes-and-other-fun-stuff>#</a></h2><p>Karpathy kicks off the lecture with a crash course in basic calculus, focusing specifically on derivatives. Now, if you&rsquo;re like me and calculus brings back memories of late-night study sessions and too much caffeine, fear not! This foundational knowledge is essential for understanding how neural networks (NNs) function.</p><p>Essentially, I&rsquo;m trying to wrap my head around how changing weights and biases‚Äîthe key parameters of NNs‚Äîaffect the output. In other words, derivatives help me figure out how much each weight in the network impacts the final output.</p><p><strong>Key takeaways:</strong></p><ul><li><strong>Derivatives and Impact:</strong> The derivative of the output with respect to each leaf node in the network shows me how much influence those nodes have on the overall output. This is crucial for tuning the NN effectively.</li><li><strong>Backpropagation Visualized:</strong> Karpathy walks through a detailed example of backpropagation using a pseudo neural network. This step-by-step process helps me visualize how backpropagation works and how chain rule derivatives come into play.</li><li><strong>Activation Functions Galore:</strong> I also touched on activation functions like <code>tanh</code> and <code>sigmoid</code>, which are the magic sauce that helps NNs make decisions.</li><li><strong>Automating Backpropagation:</strong> The lecture discusses how to algorithmically automate backpropagation and the importance of topological sort to ensure that I don&rsquo;t backpropagate until the forward pass is complete.</li><li><strong>Level of Detail:</strong> Depending on my masochistic tendencies (or dedication to understanding every little detail), I can either implement the <code>tanh</code> function directly or break it down into its individual components.</li><li><strong>PyTorch to the Rescue:</strong> PyTorch offers some handy abstractions to make life easier. For instance, its tensors are <code>float32</code> by default, but I can cast them to <code>.double()</code> for <code>float64</code> precision, matching Python&rsquo;s default floating-point precision. Also, I need to enable gradient calculation explicitly for leaf nodes (<code>requires_grad=True</code>), as they don&rsquo;t do it by default for efficiency reasons.</li></ul><p>Here&rsquo;s a snippet showing this in action:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>Casting the tensors as doubles and enabling gradient calculation
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl><span class=n>x1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>([</span><span class=mf>2.0</span><span class=p>])</span><span class=o>.</span><span class=n>double</span><span class=p>()</span>                <span class=p>;</span> <span class=n>x1</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=n>x2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>([</span><span class=mf>0.0</span><span class=p>])</span><span class=o>.</span><span class=n>double</span><span class=p>()</span>                <span class=p>;</span> <span class=n>x2</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=n>w1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>([</span><span class=o>-</span><span class=mf>3.0</span><span class=p>])</span><span class=o>.</span><span class=n>double</span><span class=p>()</span>               <span class=p>;</span> <span class=n>w1</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=n>w2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>([</span><span class=mf>1.0</span><span class=p>])</span><span class=o>.</span><span class=n>double</span><span class=p>()</span>                <span class=p>;</span> <span class=n>w2</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>([</span><span class=mf>6.8813735870195432</span><span class=p>])</span><span class=o>.</span><span class=n>double</span><span class=p>()</span>  <span class=p>;</span> <span class=n>b</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Forward pass calculation</span>
</span></span><span class=line><span class=cl><span class=n>n</span> <span class=o>=</span> <span class=n>x1</span><span class=o>*</span><span class=n>w1</span> <span class=o>+</span> <span class=n>x2</span><span class=o>*</span><span class=n>w2</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl><span class=n>o</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Output (o): </span><span class=si>{</span><span class=n>o</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Backward pass (calculating gradients)</span>
</span></span><span class=line><span class=cl><span class=n>o</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;--- Gradients ---&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;x2 grad: </span><span class=si>{</span><span class=n>x2</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;w2 grad: </span><span class=si>{</span><span class=n>w2</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;x1 grad: </span><span class=si>{</span><span class=n>x1</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;w1 grad: </span><span class=si>{</span><span class=n>w1</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;b grad: </span><span class=si>{</span><span class=n>b</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span> <span class=c1># Added bias gradient display</span>
</span></span></code></pre></div><hr><h2 id=building-blocks-neurons-layers-and-mlps>Building Blocks: Neurons, Layers, and MLPs<a hidden class=anchor aria-hidden=true href=#building-blocks-neurons-layers-and-mlps>#</a></h2><p>I then dove into the class definitions of a single neuron, a layer of neurons, and a Multi-Layer Perceptron (MLP). This section is all about understanding the fundamental building blocks of NNs and how they interconnect and work together.</p><h3 id=main-takeaways>Main takeaways:<a hidden class=anchor aria-hidden=true href=#main-takeaways>#</a></h3><p>Understanding the components is one thing; training them is another!</p><ul><li><strong>The Learning Rate Dilemma:</strong> One of the critical aspects of training NNs is choosing the appropriate learning rate. It&rsquo;s a bit of a Goldilocks problem: too large a step size and I might overshoot the optimal minimum loss; too small and training will be painfully slow (or get stuck in local minima).</li><li><strong>Pro Tip - Zeroing the Grads:</strong> A key step <em>before</em> backpropagation in each training iteration in PyTorch is to zero out the gradients from the previous step (<code>p.grad = 0.0</code>). If I forget this, PyTorch accumulates gradients across iterations, leading to incorrect updates and a lot of head-scratching.</li></ul><p>Here&rsquo;s a basic training loop structure illustrating this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Assuming &#39;n&#39; is our MLP model, &#39;xs&#39; are inputs, &#39;ys&#39; are target outputs</span>
</span></span><span class=line><span class=cl><span class=c1># And we have defined a loss function (e.g., Mean Squared Error implicitly below)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>0.05</span>
</span></span><span class=line><span class=cl><span class=n>epochs</span> <span class=o>=</span> <span class=mi>20</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1># Forward pass: Get predictions for all inputs</span>
</span></span><span class=line><span class=cl>  <span class=n>ypred</span> <span class=o>=</span> <span class=p>[</span><span class=n>n</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>xs</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=c1># Calculate loss (sum of squared errors example)</span>
</span></span><span class=line><span class=cl>  <span class=n>loss</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>((</span><span class=n>yout</span> <span class=o>-</span> <span class=n>ygt</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span> <span class=k>for</span> <span class=n>ygt</span><span class=p>,</span> <span class=n>yout</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>ys</span><span class=p>,</span> <span class=n>ypred</span><span class=p>))</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1># Backward pass</span>
</span></span><span class=line><span class=cl>  <span class=c1># &gt;&gt;&gt; Crucial Step: Zero the gradients before calculating new ones &lt;&lt;&lt;</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>n</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=o>=</span> <span class=mf>0.0</span> 
</span></span><span class=line><span class=cl>  <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span> <span class=c1># Calculate gradients for this batch</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1># Update weights and biases</span>
</span></span><span class=line><span class=cl>  <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span> <span class=c1># Temporarily disable gradient tracking for updates</span>
</span></span><span class=line><span class=cl>      <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>n</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>p</span><span class=o>.</span><span class=n>data</span> <span class=o>+=</span> <span class=o>-</span><span class=n>learning_rate</span> <span class=o>*</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=c1># Gradient descent step</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Epoch </span><span class=si>{</span><span class=n>k</span><span class=si>}</span><span class=s2>, Loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span> <span class=c1># Use .item() to get Python number</span>
</span></span></code></pre></div><hr><h2 id=summing-it-up>Summing It Up<a hidden class=anchor aria-hidden=true href=#summing-it-up>#</a></h2><p>In summary, this first dive covered how neural networks are structured, how to calculate their loss (how wrong they are), and how to use backpropagation (leveraging the power of derivatives via the chain rule, implemented as gradient descent) to minimize this loss and make the network learn. I explored the <code>tanh</code> activation function, built the fundamental components of a neural network from scratch, and got my hands dirty with PyTorch basics.</p><p>Stay tuned for more posts in this series as I continue my journey from zero to hero in the world of AI and ML! And remember, in the immortal words of Karpathy (and perhaps a few stressed-out students), <strong>&ldquo;Happy learning, and may your gradients always descend!&rdquo;</strong></p><hr></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/ai/>AI</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Sukhvir's blog</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>