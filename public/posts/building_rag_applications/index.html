<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Sukhvir's blog</title>
<meta name=keywords content><meta name=description content="For the last year, I&rsquo;ve spent a lot of time developing various AI use cases and applications within my organization. And I&rsquo;ve learned a lot during this process. Majority of the applications that are built are around robotic process automation, feature extraction, and rag. Talking about rag, let&rsquo;s focus on rag in this particular article. Here are some of the few lessons that I&rsquo;ve learned. Here are the few things that I found interesting and just some general comments that I&rsquo;d like to tell you all about. So firstly, something that I think we all know, something you must have already heard about but I just want to reinforce and stress that this is, I&rsquo;ve not had a unique experience here and I&rsquo;ll face the same problem that most do. 80%, I would say minimum is the amount of time you spend just cleaning up the data and make sure data is correct before you even get to any kind of AI process use case development and code development and application development. That takes a majority of time. In a RAG use case, for most organization, yes, there are multi-modal RAGs out there, there are fancy tools available that you can watch on YouTube, but for most organizations, you&rsquo;re going for traditional data set which may include things like PowerPoints, Excels, DocX files, PDFs, these are what you&rsquo;re looking for. The problem here is most of the content inside these files is generated for human beings to look at. Of course, it&rsquo;s an office environment and that will be the case. But most content that is generated for human eyes is not necessarily designed for LLM consumption. Taking PowerPoints as an example, most of the PowerPoints are not text-heavy. They&rsquo;re visuals-heavy. They&rsquo;re infographics. There are also a lot of processes, process-centered operating procedures, policies and guidelines and related documents that most organizations have, they would like to put a rag chatbot on. They contain visual elements like infographics that may contain information within that graphic that is not necessarily reflected in the text of the document. In which case when you try to feed it to something like a large language model using and sort of parsing that data set, those that information will be lost. Only the text when you try to feed it to something like a large language model using and sort of parsing that data set, those, that information will be lost, only the text will be retained. We can certainly be, we can certainly lead to inferior products on the other end. The other thing that I&rsquo;ve quite learned is that there is a lot of value in sort of communicating to the organization, not the organization, but to the business users within the organization about the non-deterministic and stochastic nature of the LLM. It&rsquo;s important for them to understand that if they ask the same question five times, they will get five different answers. Of course, we can take a lot of measures to avoid hallucinations. I&rsquo;m not talking about hallucinations as such but they would not get a deterministic answer every single time. This is kind of important especially in a chatbot sort of scenario to sort of explain to the users that this is a built-in feature of the system not a bug. Speaking of rag implementation itself I think when it comes to the ETL pipeline or the data loading pipeline for a rag use case as an example, the very first step that you do is you&rsquo;re going to have to pass those documents. Like I said, there are four primary sets of documents that most organizations will be dealing with, PowerPoint, Excels, DocX files, and PDFs. So the first step is really to parse them and to extract all the content within it in a sort of structured way. Typically we use Markdown for this. We tried many, many different parsers. I think the best one that sort of does the job most reliably and most effectively and sort of respects where our data lives is Azure Document Intelligence. We have our data boundaries that is held between the Swiss data center within our known data center for the Azure environment, cloud environment that we have. Therefore we can put up to confidential data within this environment, which complies with our safety, with our classification policy and security policies. So given that, either we could use on-prem document process, or we could use a Cloud Service that&rsquo;s hosted on Azure. Azure Document Intelligence seemed like the best of the class here. The other ones that we tried were dokling, and we also tried Magnum, I think it&rsquo;s called. But I think overall document intelligence was the best in retaining the layout of the documents and also extracting all of the content from the document in a structured way in a mark down format. So this is the one that we decided to use. As I mentioned, obviously, the visual elements are kind of lost. But here now, lately, we started experimenting with using document intelligence to pass the content. And document intelligence naturally captures the fact that there are figures in a particular area, and it will mark down with a figure tag. Whenever that figure tag is detected, we literally carve after a figure and send it to GPT-4 or Vision Model to summarize what is being conveyed by the figure and put it inside the document itself. So we are starting to now look at Vision-enabled document intelligence to sort of capture all of the information of a given document within a markdown file as an output. So document intelligence has really been a winning factor here for us. Then comes chunking. Once you have your past documents, you need to chunk them in a meaningful way. And there are many, many, many chunking strategies out there. There is character splitting, recursive character splitting, the semantic splitting, many, many different ways to do the chunking and splitting. I think what I&rsquo;ve found best especially for office related documents and policies and procedures, these are nicely structured for human eyes in a sort of report style format. There are headings and there are sections and each section contains a unit of information that is contained within that particular section. So for me, what I found best was to split or to chunk these past documents by section. So by using Markdown headers, H1, H2, H3 headers as a way to split these documents using Langchane&rsquo;s Markdown header splitter as the chunking strategy here. Another thing I think was important during chunking process was to also include in the metadata of each chunk the source of the document, the URL, where the document was searched from, the title of the document, the headings as the metadata of a particular chunk, the page number the chunk came from within the source document. So those four things are included in the chunking process. Next thing that I guess a bit more advanced, RAG systems do is keyword generation because we can do the vector search and the retrieval using both the embeddings or the vector similarity, but also using keywords. So the next step that I do, or what we did for our RAG was to send our chunks to a GPT-4.0 model to generate five to six keywords that represent what is being conveyed in that particular chunk. This is also included in the metadata of a chunk. Once all of this is done, we&rsquo;ve done the parsing of the document using document intelligence. We&rsquo;ve done the chunking using the Markdown headers player. Chunking method, we generated some metadata, we generated some keywords for it. Then we vectorized it using a vector database. Here are some of the lessons learned. Also is the fact that if you are able to compartmentalize the vector database into different collections of different topics, that helps with retrieval as well. Putting everything up into a single bucket while it&rsquo;s okay, I found the performance to be better by sort of containerizing, hey, this is&ndash; these set of documents talk about travel-related policies. These set of documents talk about maternity leave related or travel or event-related policies and things like that. So you split it up by topic. So you can retrieve chunks and documents and objects from a particular topic based on a user&rsquo;s question. If the user&rsquo;s question is specific to a topic, you don&rsquo;t need to retrieve irrelevant chunks or you don&rsquo;t need to even search for irrelevant within irrelevant chunks from other topics. You only force in a particular topic. If a user&rsquo;s question involves multiple topics, then you sort of query those multiple topics and retrieve relevant chunks for it. So by compartmentalizing the collections by topic, was a helpful strategy. Another thing for your ETL pipeline that is also good to make sure, especially in a corporate setting with these source documents, these policies may regularly get updated, so how do you ensure your rag chatbot reveals the right answer all the time is to also create a monitoring of the file sources that you have. So things like tracking file modifications, deletions, and additions. And be able to handle that. So if a file is modified or added, if the file is modified, find the chunks that you have in your vector database, destroy them, and treat it as if it&rsquo;s a new file. So parse it, chunk it, and add it again. If a file is new file, do the same thing. If the file has been deleted, simply to remove all the chunks from your vector database. The way we decided to go by doing that was to maintain a state file, which sort of captured the files that have been processed. The previous ETL pipeline was run and any new files since then on a nightly basis is checked. Any new files since then I then just updates the state and then you take relevant actions accordingly. One perhaps another caution point here is where does your source data live? Do you read the golden source, or the golden copy of these files, or do you create a copy of these files, and work on those copies? How do you ensure the copies remain true to the golden source? All of these things considerations must be taken care of. We used SharePoint online folders as a repository to collect the data from for our ETL pipelines. But again, here in a corporate settings, especially in a Microsoft environment, people are starting to use SharePoint Online web pages and sites to store information and departmental policies and procedures in sort of web pages. These are dot ASP pages and perhaps building a connector to sort of fetch the data off of a particular page. Things like SharePoint on-prem, things like shared drives, all of these sources are valid and should be considered for onboarding into your rack pipeline. The other thing that we did for something else that we noticed during document intelligence while it does a really, really good job, sometimes, especially for our corporate template of Word documents, Excel documents and PowerPoint documents, we noticed that when document intelligence was sort of passing them, it did not identify all the headers correctly within the document. It identified our root header, our very first title of the document as an H1 header and then sort of put everything else, all the other content within that. It did not then identify H2 headers and H3 headers. This is a problem when you&rsquo;re trying to chunk because your entire document becomes one chunk. This issue was done at random and perhaps it was linked to the corporate style of the documents that we have our own templates. Doing it on general word documents that don&rsquo;t comply with our corporate template, this was never an issue, was only an issue. So there&rsquo;s don&rsquo;t comply with our corporate template. This was never an issue, was only an issue. So there&rsquo;s something to do with our corporate template. But something like this might be of useful use of to you as well, and something for you to think about. Okay, moving on from chunking an ETL pipeline to the RAG process itself. So for example, we did a meeting services related at a rat chat pod, the meeting services department in an organization that tracks, that manages meetings, travels, event organization, meeting room bookings, catering, restaurant bookings, and all of these kind of stuff for different events and the meetings that we hold within our organization. And they have lots of policies and procedures around this and guidelines for users to book some of these services or to do some of these things. So as a rat chat bot, on top of the ETL that we built was, we would take a user&rsquo;s question. And then the first thing that we do is we sort of classify the user&rsquo;s question as to what topics is the user asking about. So we take the user&rsquo;s question, send it to an LLM and with a prompt saying, user&rsquo;s question must comply with one or many of these particular topics. If it&rsquo;s outside those topics, then the user&rsquo;s question is out of scope for this right chat box. Simply reply back to the user saying, we do not have relevant information for that particular question, please contact the meeting services team directly with your video inquiry. So if the user asks questions like, what is today&rsquo;s weather? This is outside the scope of the RAC Chatbot, and they&rsquo;ll get a standard stock standard reply. But otherwise, we&rsquo;ll ask the LLM to categorize users&rsquo; question into one or multiple topics that we have within our collection within our vector database. We&rsquo;ll collect those topics and there&rsquo;ll be a metadata field that will pass to the to our retriever so that it can retrieve from the from the right collection. The other thing that I also do with the user&rsquo;s query is sort of rephrase that query. So if you have a memory built in where the user may ask, I want to&ndash; what is the seating capacity of this particular room? And in the next question, they ask, oh, does that room support hybrid meetings? The LLM needs&ndash; the RAG service needs to know what that is referring to, the word that, and the second question, what what that is referring to, the word that in the second question, what is that even referring to? So to be able to fill in that gap, it needs the history of messages that have been passed. So we need to reformulate users&rsquo; question into a meaningful way so that the LLM can then respond. So we rephrase the query to take in previous queries. In context to fill and fill up all those gaps. Then what we do is we send, we create keywords for users query as well, and then we do, we generate embeddings for users question, and then we do sort of retrieval based on the embeddings and the keywords similarities search as well. We do BM25 retrieval here. Then we pass the retreat chunks. You can select how many, plus five, plus 10, and the user&rsquo;s question to the LLM and pass all of that information to the LLM to generate a coherent answer. As per the RAG things, one of the things that we do provide back to the user as a response citations, the titles, the page numbers as well. Another thing we&rsquo;ve recently added to our to our rag chat bot is a re-ranker and this has been game changer. We&rsquo;ve been known while the LLM does a GPD 4.0 or less the LLM specifically does a really really good job with the jumbled sort of chunks that it is sent we sent top 10 but then not necessarily in the order that you would like some of the more sometimes sometimes irrelevant chunks are above good chunks are in the middle or at the bottom what a re-ranker is does is it does a really really good job in re-priizing and reorganizing chunks. So we&rsquo;ve now started doing retrieving up to 30 or 40 chunks from the from the retriever, giving it to the re-ranker to pick out the top 10 and give those top 10 in a prioritized manner to the LLM. And you get a much better higher quality sort of response on the other end as a result of it as well. One thing I would also talk about is the&ndash;"><meta name=author content="Sukhvir Notra"><link rel=canonical href=http://localhost:1313/posts/building_rag_applications/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon.png><link rel=apple-touch-icon href=http://localhost:1313/favicon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/building_rag_applications/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/building_rag_applications/"><meta property="og:site_name" content="Sukhvir's blog"><meta property="og:title" content="Sukhvir's blog"><meta property="og:description" content="For the last year, I’ve spent a lot of time developing various AI use cases and applications within my organization. And I’ve learned a lot during this process. Majority of the applications that are built are around robotic process automation, feature extraction, and rag. Talking about rag, let’s focus on rag in this particular article. Here are some of the few lessons that I’ve learned. Here are the few things that I found interesting and just some general comments that I’d like to tell you all about. So firstly, something that I think we all know, something you must have already heard about but I just want to reinforce and stress that this is, I’ve not had a unique experience here and I’ll face the same problem that most do. 80%, I would say minimum is the amount of time you spend just cleaning up the data and make sure data is correct before you even get to any kind of AI process use case development and code development and application development. That takes a majority of time. In a RAG use case, for most organization, yes, there are multi-modal RAGs out there, there are fancy tools available that you can watch on YouTube, but for most organizations, you’re going for traditional data set which may include things like PowerPoints, Excels, DocX files, PDFs, these are what you’re looking for. The problem here is most of the content inside these files is generated for human beings to look at. Of course, it’s an office environment and that will be the case. But most content that is generated for human eyes is not necessarily designed for LLM consumption. Taking PowerPoints as an example, most of the PowerPoints are not text-heavy. They’re visuals-heavy. They’re infographics. There are also a lot of processes, process-centered operating procedures, policies and guidelines and related documents that most organizations have, they would like to put a rag chatbot on. They contain visual elements like infographics that may contain information within that graphic that is not necessarily reflected in the text of the document. In which case when you try to feed it to something like a large language model using and sort of parsing that data set, those that information will be lost. Only the text when you try to feed it to something like a large language model using and sort of parsing that data set, those, that information will be lost, only the text will be retained. We can certainly be, we can certainly lead to inferior products on the other end. The other thing that I’ve quite learned is that there is a lot of value in sort of communicating to the organization, not the organization, but to the business users within the organization about the non-deterministic and stochastic nature of the LLM. It’s important for them to understand that if they ask the same question five times, they will get five different answers. Of course, we can take a lot of measures to avoid hallucinations. I’m not talking about hallucinations as such but they would not get a deterministic answer every single time. This is kind of important especially in a chatbot sort of scenario to sort of explain to the users that this is a built-in feature of the system not a bug. Speaking of rag implementation itself I think when it comes to the ETL pipeline or the data loading pipeline for a rag use case as an example, the very first step that you do is you’re going to have to pass those documents. Like I said, there are four primary sets of documents that most organizations will be dealing with, PowerPoint, Excels, DocX files, and PDFs. So the first step is really to parse them and to extract all the content within it in a sort of structured way. Typically we use Markdown for this. We tried many, many different parsers. I think the best one that sort of does the job most reliably and most effectively and sort of respects where our data lives is Azure Document Intelligence. We have our data boundaries that is held between the Swiss data center within our known data center for the Azure environment, cloud environment that we have. Therefore we can put up to confidential data within this environment, which complies with our safety, with our classification policy and security policies. So given that, either we could use on-prem document process, or we could use a Cloud Service that’s hosted on Azure. Azure Document Intelligence seemed like the best of the class here. The other ones that we tried were dokling, and we also tried Magnum, I think it’s called. But I think overall document intelligence was the best in retaining the layout of the documents and also extracting all of the content from the document in a structured way in a mark down format. So this is the one that we decided to use. As I mentioned, obviously, the visual elements are kind of lost. But here now, lately, we started experimenting with using document intelligence to pass the content. And document intelligence naturally captures the fact that there are figures in a particular area, and it will mark down with a figure tag. Whenever that figure tag is detected, we literally carve after a figure and send it to GPT-4 or Vision Model to summarize what is being conveyed by the figure and put it inside the document itself. So we are starting to now look at Vision-enabled document intelligence to sort of capture all of the information of a given document within a markdown file as an output. So document intelligence has really been a winning factor here for us. Then comes chunking. Once you have your past documents, you need to chunk them in a meaningful way. And there are many, many, many chunking strategies out there. There is character splitting, recursive character splitting, the semantic splitting, many, many different ways to do the chunking and splitting. I think what I’ve found best especially for office related documents and policies and procedures, these are nicely structured for human eyes in a sort of report style format. There are headings and there are sections and each section contains a unit of information that is contained within that particular section. So for me, what I found best was to split or to chunk these past documents by section. So by using Markdown headers, H1, H2, H3 headers as a way to split these documents using Langchane’s Markdown header splitter as the chunking strategy here. Another thing I think was important during chunking process was to also include in the metadata of each chunk the source of the document, the URL, where the document was searched from, the title of the document, the headings as the metadata of a particular chunk, the page number the chunk came from within the source document. So those four things are included in the chunking process. Next thing that I guess a bit more advanced, RAG systems do is keyword generation because we can do the vector search and the retrieval using both the embeddings or the vector similarity, but also using keywords. So the next step that I do, or what we did for our RAG was to send our chunks to a GPT-4.0 model to generate five to six keywords that represent what is being conveyed in that particular chunk. This is also included in the metadata of a chunk. Once all of this is done, we’ve done the parsing of the document using document intelligence. We’ve done the chunking using the Markdown headers player. Chunking method, we generated some metadata, we generated some keywords for it. Then we vectorized it using a vector database. Here are some of the lessons learned. Also is the fact that if you are able to compartmentalize the vector database into different collections of different topics, that helps with retrieval as well. Putting everything up into a single bucket while it’s okay, I found the performance to be better by sort of containerizing, hey, this is– these set of documents talk about travel-related policies. These set of documents talk about maternity leave related or travel or event-related policies and things like that. So you split it up by topic. So you can retrieve chunks and documents and objects from a particular topic based on a user’s question. If the user’s question is specific to a topic, you don’t need to retrieve irrelevant chunks or you don’t need to even search for irrelevant within irrelevant chunks from other topics. You only force in a particular topic. If a user’s question involves multiple topics, then you sort of query those multiple topics and retrieve relevant chunks for it. So by compartmentalizing the collections by topic, was a helpful strategy. Another thing for your ETL pipeline that is also good to make sure, especially in a corporate setting with these source documents, these policies may regularly get updated, so how do you ensure your rag chatbot reveals the right answer all the time is to also create a monitoring of the file sources that you have. So things like tracking file modifications, deletions, and additions. And be able to handle that. So if a file is modified or added, if the file is modified, find the chunks that you have in your vector database, destroy them, and treat it as if it’s a new file. So parse it, chunk it, and add it again. If a file is new file, do the same thing. If the file has been deleted, simply to remove all the chunks from your vector database. The way we decided to go by doing that was to maintain a state file, which sort of captured the files that have been processed. The previous ETL pipeline was run and any new files since then on a nightly basis is checked. Any new files since then I then just updates the state and then you take relevant actions accordingly. One perhaps another caution point here is where does your source data live? Do you read the golden source, or the golden copy of these files, or do you create a copy of these files, and work on those copies? How do you ensure the copies remain true to the golden source? All of these things considerations must be taken care of. We used SharePoint online folders as a repository to collect the data from for our ETL pipelines. But again, here in a corporate settings, especially in a Microsoft environment, people are starting to use SharePoint Online web pages and sites to store information and departmental policies and procedures in sort of web pages. These are dot ASP pages and perhaps building a connector to sort of fetch the data off of a particular page. Things like SharePoint on-prem, things like shared drives, all of these sources are valid and should be considered for onboarding into your rack pipeline. The other thing that we did for something else that we noticed during document intelligence while it does a really, really good job, sometimes, especially for our corporate template of Word documents, Excel documents and PowerPoint documents, we noticed that when document intelligence was sort of passing them, it did not identify all the headers correctly within the document. It identified our root header, our very first title of the document as an H1 header and then sort of put everything else, all the other content within that. It did not then identify H2 headers and H3 headers. This is a problem when you’re trying to chunk because your entire document becomes one chunk. This issue was done at random and perhaps it was linked to the corporate style of the documents that we have our own templates. Doing it on general word documents that don’t comply with our corporate template, this was never an issue, was only an issue. So there’s don’t comply with our corporate template. This was never an issue, was only an issue. So there’s something to do with our corporate template. But something like this might be of useful use of to you as well, and something for you to think about. Okay, moving on from chunking an ETL pipeline to the RAG process itself. So for example, we did a meeting services related at a rat chat pod, the meeting services department in an organization that tracks, that manages meetings, travels, event organization, meeting room bookings, catering, restaurant bookings, and all of these kind of stuff for different events and the meetings that we hold within our organization. And they have lots of policies and procedures around this and guidelines for users to book some of these services or to do some of these things. So as a rat chat bot, on top of the ETL that we built was, we would take a user’s question. And then the first thing that we do is we sort of classify the user’s question as to what topics is the user asking about. So we take the user’s question, send it to an LLM and with a prompt saying, user’s question must comply with one or many of these particular topics. If it’s outside those topics, then the user’s question is out of scope for this right chat box. Simply reply back to the user saying, we do not have relevant information for that particular question, please contact the meeting services team directly with your video inquiry. So if the user asks questions like, what is today’s weather? This is outside the scope of the RAC Chatbot, and they’ll get a standard stock standard reply. But otherwise, we’ll ask the LLM to categorize users’ question into one or multiple topics that we have within our collection within our vector database. We’ll collect those topics and there’ll be a metadata field that will pass to the to our retriever so that it can retrieve from the from the right collection. The other thing that I also do with the user’s query is sort of rephrase that query. So if you have a memory built in where the user may ask, I want to– what is the seating capacity of this particular room? And in the next question, they ask, oh, does that room support hybrid meetings? The LLM needs– the RAG service needs to know what that is referring to, the word that, and the second question, what what that is referring to, the word that in the second question, what is that even referring to? So to be able to fill in that gap, it needs the history of messages that have been passed. So we need to reformulate users’ question into a meaningful way so that the LLM can then respond. So we rephrase the query to take in previous queries. In context to fill and fill up all those gaps. Then what we do is we send, we create keywords for users query as well, and then we do, we generate embeddings for users question, and then we do sort of retrieval based on the embeddings and the keywords similarities search as well. We do BM25 retrieval here. Then we pass the retreat chunks. You can select how many, plus five, plus 10, and the user’s question to the LLM and pass all of that information to the LLM to generate a coherent answer. As per the RAG things, one of the things that we do provide back to the user as a response citations, the titles, the page numbers as well. Another thing we’ve recently added to our to our rag chat bot is a re-ranker and this has been game changer. We’ve been known while the LLM does a GPD 4.0 or less the LLM specifically does a really really good job with the jumbled sort of chunks that it is sent we sent top 10 but then not necessarily in the order that you would like some of the more sometimes sometimes irrelevant chunks are above good chunks are in the middle or at the bottom what a re-ranker is does is it does a really really good job in re-priizing and reorganizing chunks. So we’ve now started doing retrieving up to 30 or 40 chunks from the from the retriever, giving it to the re-ranker to pick out the top 10 and give those top 10 in a prioritized manner to the LLM. And you get a much better higher quality sort of response on the other end as a result of it as well. One thing I would also talk about is the–"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="For the last year, I&rsquo;ve spent a lot of time developing various AI use cases and applications within my organization. And I&rsquo;ve learned a lot during this process. Majority of the applications that are built are around robotic process automation, feature extraction, and rag. Talking about rag, let&rsquo;s focus on rag in this particular article. Here are some of the few lessons that I&rsquo;ve learned. Here are the few things that I found interesting and just some general comments that I&rsquo;d like to tell you all about. So firstly, something that I think we all know, something you must have already heard about but I just want to reinforce and stress that this is, I&rsquo;ve not had a unique experience here and I&rsquo;ll face the same problem that most do. 80%, I would say minimum is the amount of time you spend just cleaning up the data and make sure data is correct before you even get to any kind of AI process use case development and code development and application development. That takes a majority of time. In a RAG use case, for most organization, yes, there are multi-modal RAGs out there, there are fancy tools available that you can watch on YouTube, but for most organizations, you&rsquo;re going for traditional data set which may include things like PowerPoints, Excels, DocX files, PDFs, these are what you&rsquo;re looking for. The problem here is most of the content inside these files is generated for human beings to look at. Of course, it&rsquo;s an office environment and that will be the case. But most content that is generated for human eyes is not necessarily designed for LLM consumption. Taking PowerPoints as an example, most of the PowerPoints are not text-heavy. They&rsquo;re visuals-heavy. They&rsquo;re infographics. There are also a lot of processes, process-centered operating procedures, policies and guidelines and related documents that most organizations have, they would like to put a rag chatbot on. They contain visual elements like infographics that may contain information within that graphic that is not necessarily reflected in the text of the document. In which case when you try to feed it to something like a large language model using and sort of parsing that data set, those that information will be lost. Only the text when you try to feed it to something like a large language model using and sort of parsing that data set, those, that information will be lost, only the text will be retained. We can certainly be, we can certainly lead to inferior products on the other end. The other thing that I&rsquo;ve quite learned is that there is a lot of value in sort of communicating to the organization, not the organization, but to the business users within the organization about the non-deterministic and stochastic nature of the LLM. It&rsquo;s important for them to understand that if they ask the same question five times, they will get five different answers. Of course, we can take a lot of measures to avoid hallucinations. I&rsquo;m not talking about hallucinations as such but they would not get a deterministic answer every single time. This is kind of important especially in a chatbot sort of scenario to sort of explain to the users that this is a built-in feature of the system not a bug. Speaking of rag implementation itself I think when it comes to the ETL pipeline or the data loading pipeline for a rag use case as an example, the very first step that you do is you&rsquo;re going to have to pass those documents. Like I said, there are four primary sets of documents that most organizations will be dealing with, PowerPoint, Excels, DocX files, and PDFs. So the first step is really to parse them and to extract all the content within it in a sort of structured way. Typically we use Markdown for this. We tried many, many different parsers. I think the best one that sort of does the job most reliably and most effectively and sort of respects where our data lives is Azure Document Intelligence. We have our data boundaries that is held between the Swiss data center within our known data center for the Azure environment, cloud environment that we have. Therefore we can put up to confidential data within this environment, which complies with our safety, with our classification policy and security policies. So given that, either we could use on-prem document process, or we could use a Cloud Service that&rsquo;s hosted on Azure. Azure Document Intelligence seemed like the best of the class here. The other ones that we tried were dokling, and we also tried Magnum, I think it&rsquo;s called. But I think overall document intelligence was the best in retaining the layout of the documents and also extracting all of the content from the document in a structured way in a mark down format. So this is the one that we decided to use. As I mentioned, obviously, the visual elements are kind of lost. But here now, lately, we started experimenting with using document intelligence to pass the content. And document intelligence naturally captures the fact that there are figures in a particular area, and it will mark down with a figure tag. Whenever that figure tag is detected, we literally carve after a figure and send it to GPT-4 or Vision Model to summarize what is being conveyed by the figure and put it inside the document itself. So we are starting to now look at Vision-enabled document intelligence to sort of capture all of the information of a given document within a markdown file as an output. So document intelligence has really been a winning factor here for us. Then comes chunking. Once you have your past documents, you need to chunk them in a meaningful way. And there are many, many, many chunking strategies out there. There is character splitting, recursive character splitting, the semantic splitting, many, many different ways to do the chunking and splitting. I think what I&rsquo;ve found best especially for office related documents and policies and procedures, these are nicely structured for human eyes in a sort of report style format. There are headings and there are sections and each section contains a unit of information that is contained within that particular section. So for me, what I found best was to split or to chunk these past documents by section. So by using Markdown headers, H1, H2, H3 headers as a way to split these documents using Langchane&rsquo;s Markdown header splitter as the chunking strategy here. Another thing I think was important during chunking process was to also include in the metadata of each chunk the source of the document, the URL, where the document was searched from, the title of the document, the headings as the metadata of a particular chunk, the page number the chunk came from within the source document. So those four things are included in the chunking process. Next thing that I guess a bit more advanced, RAG systems do is keyword generation because we can do the vector search and the retrieval using both the embeddings or the vector similarity, but also using keywords. So the next step that I do, or what we did for our RAG was to send our chunks to a GPT-4.0 model to generate five to six keywords that represent what is being conveyed in that particular chunk. This is also included in the metadata of a chunk. Once all of this is done, we&rsquo;ve done the parsing of the document using document intelligence. We&rsquo;ve done the chunking using the Markdown headers player. Chunking method, we generated some metadata, we generated some keywords for it. Then we vectorized it using a vector database. Here are some of the lessons learned. Also is the fact that if you are able to compartmentalize the vector database into different collections of different topics, that helps with retrieval as well. Putting everything up into a single bucket while it&rsquo;s okay, I found the performance to be better by sort of containerizing, hey, this is&ndash; these set of documents talk about travel-related policies. These set of documents talk about maternity leave related or travel or event-related policies and things like that. So you split it up by topic. So you can retrieve chunks and documents and objects from a particular topic based on a user&rsquo;s question. If the user&rsquo;s question is specific to a topic, you don&rsquo;t need to retrieve irrelevant chunks or you don&rsquo;t need to even search for irrelevant within irrelevant chunks from other topics. You only force in a particular topic. If a user&rsquo;s question involves multiple topics, then you sort of query those multiple topics and retrieve relevant chunks for it. So by compartmentalizing the collections by topic, was a helpful strategy. Another thing for your ETL pipeline that is also good to make sure, especially in a corporate setting with these source documents, these policies may regularly get updated, so how do you ensure your rag chatbot reveals the right answer all the time is to also create a monitoring of the file sources that you have. So things like tracking file modifications, deletions, and additions. And be able to handle that. So if a file is modified or added, if the file is modified, find the chunks that you have in your vector database, destroy them, and treat it as if it&rsquo;s a new file. So parse it, chunk it, and add it again. If a file is new file, do the same thing. If the file has been deleted, simply to remove all the chunks from your vector database. The way we decided to go by doing that was to maintain a state file, which sort of captured the files that have been processed. The previous ETL pipeline was run and any new files since then on a nightly basis is checked. Any new files since then I then just updates the state and then you take relevant actions accordingly. One perhaps another caution point here is where does your source data live? Do you read the golden source, or the golden copy of these files, or do you create a copy of these files, and work on those copies? How do you ensure the copies remain true to the golden source? All of these things considerations must be taken care of. We used SharePoint online folders as a repository to collect the data from for our ETL pipelines. But again, here in a corporate settings, especially in a Microsoft environment, people are starting to use SharePoint Online web pages and sites to store information and departmental policies and procedures in sort of web pages. These are dot ASP pages and perhaps building a connector to sort of fetch the data off of a particular page. Things like SharePoint on-prem, things like shared drives, all of these sources are valid and should be considered for onboarding into your rack pipeline. The other thing that we did for something else that we noticed during document intelligence while it does a really, really good job, sometimes, especially for our corporate template of Word documents, Excel documents and PowerPoint documents, we noticed that when document intelligence was sort of passing them, it did not identify all the headers correctly within the document. It identified our root header, our very first title of the document as an H1 header and then sort of put everything else, all the other content within that. It did not then identify H2 headers and H3 headers. This is a problem when you&rsquo;re trying to chunk because your entire document becomes one chunk. This issue was done at random and perhaps it was linked to the corporate style of the documents that we have our own templates. Doing it on general word documents that don&rsquo;t comply with our corporate template, this was never an issue, was only an issue. So there&rsquo;s don&rsquo;t comply with our corporate template. This was never an issue, was only an issue. So there&rsquo;s something to do with our corporate template. But something like this might be of useful use of to you as well, and something for you to think about. Okay, moving on from chunking an ETL pipeline to the RAG process itself. So for example, we did a meeting services related at a rat chat pod, the meeting services department in an organization that tracks, that manages meetings, travels, event organization, meeting room bookings, catering, restaurant bookings, and all of these kind of stuff for different events and the meetings that we hold within our organization. And they have lots of policies and procedures around this and guidelines for users to book some of these services or to do some of these things. So as a rat chat bot, on top of the ETL that we built was, we would take a user&rsquo;s question. And then the first thing that we do is we sort of classify the user&rsquo;s question as to what topics is the user asking about. So we take the user&rsquo;s question, send it to an LLM and with a prompt saying, user&rsquo;s question must comply with one or many of these particular topics. If it&rsquo;s outside those topics, then the user&rsquo;s question is out of scope for this right chat box. Simply reply back to the user saying, we do not have relevant information for that particular question, please contact the meeting services team directly with your video inquiry. So if the user asks questions like, what is today&rsquo;s weather? This is outside the scope of the RAC Chatbot, and they&rsquo;ll get a standard stock standard reply. But otherwise, we&rsquo;ll ask the LLM to categorize users&rsquo; question into one or multiple topics that we have within our collection within our vector database. We&rsquo;ll collect those topics and there&rsquo;ll be a metadata field that will pass to the to our retriever so that it can retrieve from the from the right collection. The other thing that I also do with the user&rsquo;s query is sort of rephrase that query. So if you have a memory built in where the user may ask, I want to&ndash; what is the seating capacity of this particular room? And in the next question, they ask, oh, does that room support hybrid meetings? The LLM needs&ndash; the RAG service needs to know what that is referring to, the word that, and the second question, what what that is referring to, the word that in the second question, what is that even referring to? So to be able to fill in that gap, it needs the history of messages that have been passed. So we need to reformulate users&rsquo; question into a meaningful way so that the LLM can then respond. So we rephrase the query to take in previous queries. In context to fill and fill up all those gaps. Then what we do is we send, we create keywords for users query as well, and then we do, we generate embeddings for users question, and then we do sort of retrieval based on the embeddings and the keywords similarities search as well. We do BM25 retrieval here. Then we pass the retreat chunks. You can select how many, plus five, plus 10, and the user&rsquo;s question to the LLM and pass all of that information to the LLM to generate a coherent answer. As per the RAG things, one of the things that we do provide back to the user as a response citations, the titles, the page numbers as well. Another thing we&rsquo;ve recently added to our to our rag chat bot is a re-ranker and this has been game changer. We&rsquo;ve been known while the LLM does a GPD 4.0 or less the LLM specifically does a really really good job with the jumbled sort of chunks that it is sent we sent top 10 but then not necessarily in the order that you would like some of the more sometimes sometimes irrelevant chunks are above good chunks are in the middle or at the bottom what a re-ranker is does is it does a really really good job in re-priizing and reorganizing chunks. So we&rsquo;ve now started doing retrieving up to 30 or 40 chunks from the from the retriever, giving it to the re-ranker to pick out the top 10 and give those top 10 in a prioritized manner to the LLM. And you get a much better higher quality sort of response on the other end as a result of it as well. One thing I would also talk about is the&ndash;"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"","item":"http://localhost:1313/posts/building_rag_applications/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"","name":"","description":"For the last year, I\u0026rsquo;ve spent a lot of time developing various AI use cases and applications within my organization. And I\u0026rsquo;ve learned a lot during this process. Majority of the applications that are built are around robotic process automation, feature extraction, and rag. Talking about rag, let\u0026rsquo;s focus on rag in this particular article. Here are some of the few lessons that I\u0026rsquo;ve learned. Here are the few things that I found interesting and just some general comments that I\u0026rsquo;d like to tell you all about. So firstly, something that I think we all know, something you must have already heard about but I just want to reinforce and stress that this is, I\u0026rsquo;ve not had a unique experience here and I\u0026rsquo;ll face the same problem that most do. 80%, I would say minimum is the amount of time you spend just cleaning up the data and make sure data is correct before you even get to any kind of AI process use case development and code development and application development. That takes a majority of time. In a RAG use case, for most organization, yes, there are multi-modal RAGs out there, there are fancy tools available that you can watch on YouTube, but for most organizations, you\u0026rsquo;re going for traditional data set which may include things like PowerPoints, Excels, DocX files, PDFs, these are what you\u0026rsquo;re looking for. The problem here is most of the content inside these files is generated for human beings to look at. Of course, it\u0026rsquo;s an office environment and that will be the case. But most content that is generated for human eyes is not necessarily designed for LLM consumption. Taking PowerPoints as an example, most of the PowerPoints are not text-heavy. They\u0026rsquo;re visuals-heavy. They\u0026rsquo;re infographics. There are also a lot of processes, process-centered operating procedures, policies and guidelines and related documents that most organizations have, they would like to put a rag chatbot on. They contain visual elements like infographics that may contain information within that graphic that is not necessarily reflected in the text of the document. In which case when you try to feed it to something like a large language model using and sort of parsing that data set, those that information will be lost. Only the text when you try to feed it to something like a large language model using and sort of parsing that data set, those, that information will be lost, only the text will be retained. We can certainly be, we can certainly lead to inferior products on the other end. The other thing that I\u0026rsquo;ve quite learned is that there is a lot of value in sort of communicating to the organization, not the organization, but to the business users within the organization about the non-deterministic and stochastic nature of the LLM. It\u0026rsquo;s important for them to understand that if they ask the same question five times, they will get five different answers. Of course, we can take a lot of measures to avoid hallucinations. I\u0026rsquo;m not talking about hallucinations as such but they would not get a deterministic answer every single time. This is kind of important especially in a chatbot sort of scenario to sort of explain to the users that this is a built-in feature of the system not a bug. Speaking of rag implementation itself I think when it comes to the ETL pipeline or the data loading pipeline for a rag use case as an example, the very first step that you do is you\u0026rsquo;re going to have to pass those documents. Like I said, there are four primary sets of documents that most organizations will be dealing with, PowerPoint, Excels, DocX files, and PDFs. So the first step is really to parse them and to extract all the content within it in a sort of structured way. Typically we use Markdown for this. We tried many, many different parsers. I think the best one that sort of does the job most reliably and most effectively and sort of respects where our data lives is Azure Document Intelligence. We have our data boundaries that is held between the Swiss data center within our known data center for the Azure environment, cloud environment that we have. Therefore we can put up to confidential data within this environment, which complies with our safety, with our classification policy and security policies. So given that, either we could use on-prem document process, or we could use a Cloud Service that\u0026rsquo;s hosted on Azure. Azure Document Intelligence seemed like the best of the class here. The other ones that we tried were dokling, and we also tried Magnum, I think it\u0026rsquo;s called. But I think overall document intelligence was the best in retaining the layout of the documents and also extracting all of the content from the document in a structured way in a mark down format. So this is the one that we decided to use. As I mentioned, obviously, the visual elements are kind of lost. But here now, lately, we started experimenting with using document intelligence to pass the content. And document intelligence naturally captures the fact that there are figures in a particular area, and it will mark down with a figure tag. Whenever that figure tag is detected, we literally carve after a figure and send it to GPT-4 or Vision Model to summarize what is being conveyed by the figure and put it inside the document itself. So we are starting to now look at Vision-enabled document intelligence to sort of capture all of the information of a given document within a markdown file as an output. So document intelligence has really been a winning factor here for us. Then comes chunking. Once you have your past documents, you need to chunk them in a meaningful way. And there are many, many, many chunking strategies out there. There is character splitting, recursive character splitting, the semantic splitting, many, many different ways to do the chunking and splitting. I think what I\u0026rsquo;ve found best especially for office related documents and policies and procedures, these are nicely structured for human eyes in a sort of report style format. There are headings and there are sections and each section contains a unit of information that is contained within that particular section. So for me, what I found best was to split or to chunk these past documents by section. So by using Markdown headers, H1, H2, H3 headers as a way to split these documents using Langchane\u0026rsquo;s Markdown header splitter as the chunking strategy here. Another thing I think was important during chunking process was to also include in the metadata of each chunk the source of the document, the URL, where the document was searched from, the title of the document, the headings as the metadata of a particular chunk, the page number the chunk came from within the source document. So those four things are included in the chunking process. Next thing that I guess a bit more advanced, RAG systems do is keyword generation because we can do the vector search and the retrieval using both the embeddings or the vector similarity, but also using keywords. So the next step that I do, or what we did for our RAG was to send our chunks to a GPT-4.0 model to generate five to six keywords that represent what is being conveyed in that particular chunk. This is also included in the metadata of a chunk. Once all of this is done, we\u0026rsquo;ve done the parsing of the document using document intelligence. We\u0026rsquo;ve done the chunking using the Markdown headers player. Chunking method, we generated some metadata, we generated some keywords for it. Then we vectorized it using a vector database. Here are some of the lessons learned. Also is the fact that if you are able to compartmentalize the vector database into different collections of different topics, that helps with retrieval as well. Putting everything up into a single bucket while it\u0026rsquo;s okay, I found the performance to be better by sort of containerizing, hey, this is\u0026ndash; these set of documents talk about travel-related policies. These set of documents talk about maternity leave related or travel or event-related policies and things like that. So you split it up by topic. So you can retrieve chunks and documents and objects from a particular topic based on a user\u0026rsquo;s question. If the user\u0026rsquo;s question is specific to a topic, you don\u0026rsquo;t need to retrieve irrelevant chunks or you don\u0026rsquo;t need to even search for irrelevant within irrelevant chunks from other topics. You only force in a particular topic. If a user\u0026rsquo;s question involves multiple topics, then you sort of query those multiple topics and retrieve relevant chunks for it. So by compartmentalizing the collections by topic, was a helpful strategy. Another thing for your ETL pipeline that is also good to make sure, especially in a corporate setting with these source documents, these policies may regularly get updated, so how do you ensure your rag chatbot reveals the right answer all the time is to also create a monitoring of the file sources that you have. So things like tracking file modifications, deletions, and additions. And be able to handle that. So if a file is modified or added, if the file is modified, find the chunks that you have in your vector database, destroy them, and treat it as if it\u0026rsquo;s a new file. So parse it, chunk it, and add it again. If a file is new file, do the same thing. If the file has been deleted, simply to remove all the chunks from your vector database. The way we decided to go by doing that was to maintain a state file, which sort of captured the files that have been processed. The previous ETL pipeline was run and any new files since then on a nightly basis is checked. Any new files since then I then just updates the state and then you take relevant actions accordingly. One perhaps another caution point here is where does your source data live? Do you read the golden source, or the golden copy of these files, or do you create a copy of these files, and work on those copies? How do you ensure the copies remain true to the golden source? All of these things considerations must be taken care of. We used SharePoint online folders as a repository to collect the data from for our ETL pipelines. But again, here in a corporate settings, especially in a Microsoft environment, people are starting to use SharePoint Online web pages and sites to store information and departmental policies and procedures in sort of web pages. These are dot ASP pages and perhaps building a connector to sort of fetch the data off of a particular page. Things like SharePoint on-prem, things like shared drives, all of these sources are valid and should be considered for onboarding into your rack pipeline. The other thing that we did for something else that we noticed during document intelligence while it does a really, really good job, sometimes, especially for our corporate template of Word documents, Excel documents and PowerPoint documents, we noticed that when document intelligence was sort of passing them, it did not identify all the headers correctly within the document. It identified our root header, our very first title of the document as an H1 header and then sort of put everything else, all the other content within that. It did not then identify H2 headers and H3 headers. This is a problem when you\u0026rsquo;re trying to chunk because your entire document becomes one chunk. This issue was done at random and perhaps it was linked to the corporate style of the documents that we have our own templates. Doing it on general word documents that don\u0026rsquo;t comply with our corporate template, this was never an issue, was only an issue. So there\u0026rsquo;s don\u0026rsquo;t comply with our corporate template. This was never an issue, was only an issue. So there\u0026rsquo;s something to do with our corporate template. But something like this might be of useful use of to you as well, and something for you to think about. Okay, moving on from chunking an ETL pipeline to the RAG process itself. So for example, we did a meeting services related at a rat chat pod, the meeting services department in an organization that tracks, that manages meetings, travels, event organization, meeting room bookings, catering, restaurant bookings, and all of these kind of stuff for different events and the meetings that we hold within our organization. And they have lots of policies and procedures around this and guidelines for users to book some of these services or to do some of these things. So as a rat chat bot, on top of the ETL that we built was, we would take a user\u0026rsquo;s question. And then the first thing that we do is we sort of classify the user\u0026rsquo;s question as to what topics is the user asking about. So we take the user\u0026rsquo;s question, send it to an LLM and with a prompt saying, user\u0026rsquo;s question must comply with one or many of these particular topics. If it\u0026rsquo;s outside those topics, then the user\u0026rsquo;s question is out of scope for this right chat box. Simply reply back to the user saying, we do not have relevant information for that particular question, please contact the meeting services team directly with your video inquiry. So if the user asks questions like, what is today\u0026rsquo;s weather? This is outside the scope of the RAC Chatbot, and they\u0026rsquo;ll get a standard stock standard reply. But otherwise, we\u0026rsquo;ll ask the LLM to categorize users\u0026rsquo; question into one or multiple topics that we have within our collection within our vector database. We\u0026rsquo;ll collect those topics and there\u0026rsquo;ll be a metadata field that will pass to the to our retriever so that it can retrieve from the from the right collection. The other thing that I also do with the user\u0026rsquo;s query is sort of rephrase that query. So if you have a memory built in where the user may ask, I want to\u0026ndash; what is the seating capacity of this particular room? And in the next question, they ask, oh, does that room support hybrid meetings? The LLM needs\u0026ndash; the RAG service needs to know what that is referring to, the word that, and the second question, what what that is referring to, the word that in the second question, what is that even referring to? So to be able to fill in that gap, it needs the history of messages that have been passed. So we need to reformulate users\u0026rsquo; question into a meaningful way so that the LLM can then respond. So we rephrase the query to take in previous queries. In context to fill and fill up all those gaps. Then what we do is we send, we create keywords for users query as well, and then we do, we generate embeddings for users question, and then we do sort of retrieval based on the embeddings and the keywords similarities search as well. We do BM25 retrieval here. Then we pass the retreat chunks. You can select how many, plus five, plus 10, and the user\u0026rsquo;s question to the LLM and pass all of that information to the LLM to generate a coherent answer. As per the RAG things, one of the things that we do provide back to the user as a response citations, the titles, the page numbers as well. Another thing we\u0026rsquo;ve recently added to our to our rag chat bot is a re-ranker and this has been game changer. We\u0026rsquo;ve been known while the LLM does a GPD 4.0 or less the LLM specifically does a really really good job with the jumbled sort of chunks that it is sent we sent top 10 but then not necessarily in the order that you would like some of the more sometimes sometimes irrelevant chunks are above good chunks are in the middle or at the bottom what a re-ranker is does is it does a really really good job in re-priizing and reorganizing chunks. So we\u0026rsquo;ve now started doing retrieving up to 30 or 40 chunks from the from the retriever, giving it to the re-ranker to pick out the top 10 and give those top 10 in a prioritized manner to the LLM. And you get a much better higher quality sort of response on the other end as a result of it as well. One thing I would also talk about is the\u0026ndash;\n","keywords":[],"articleBody":"For the last year, I’ve spent a lot of time developing various AI use cases and applications within my organization. And I’ve learned a lot during this process. Majority of the applications that are built are around robotic process automation, feature extraction, and rag. Talking about rag, let’s focus on rag in this particular article. Here are some of the few lessons that I’ve learned. Here are the few things that I found interesting and just some general comments that I’d like to tell you all about. So firstly, something that I think we all know, something you must have already heard about but I just want to reinforce and stress that this is, I’ve not had a unique experience here and I’ll face the same problem that most do. 80%, I would say minimum is the amount of time you spend just cleaning up the data and make sure data is correct before you even get to any kind of AI process use case development and code development and application development. That takes a majority of time. In a RAG use case, for most organization, yes, there are multi-modal RAGs out there, there are fancy tools available that you can watch on YouTube, but for most organizations, you’re going for traditional data set which may include things like PowerPoints, Excels, DocX files, PDFs, these are what you’re looking for. The problem here is most of the content inside these files is generated for human beings to look at. Of course, it’s an office environment and that will be the case. But most content that is generated for human eyes is not necessarily designed for LLM consumption. Taking PowerPoints as an example, most of the PowerPoints are not text-heavy. They’re visuals-heavy. They’re infographics. There are also a lot of processes, process-centered operating procedures, policies and guidelines and related documents that most organizations have, they would like to put a rag chatbot on. They contain visual elements like infographics that may contain information within that graphic that is not necessarily reflected in the text of the document. In which case when you try to feed it to something like a large language model using and sort of parsing that data set, those that information will be lost. Only the text when you try to feed it to something like a large language model using and sort of parsing that data set, those, that information will be lost, only the text will be retained. We can certainly be, we can certainly lead to inferior products on the other end. The other thing that I’ve quite learned is that there is a lot of value in sort of communicating to the organization, not the organization, but to the business users within the organization about the non-deterministic and stochastic nature of the LLM. It’s important for them to understand that if they ask the same question five times, they will get five different answers. Of course, we can take a lot of measures to avoid hallucinations. I’m not talking about hallucinations as such but they would not get a deterministic answer every single time. This is kind of important especially in a chatbot sort of scenario to sort of explain to the users that this is a built-in feature of the system not a bug. Speaking of rag implementation itself I think when it comes to the ETL pipeline or the data loading pipeline for a rag use case as an example, the very first step that you do is you’re going to have to pass those documents. Like I said, there are four primary sets of documents that most organizations will be dealing with, PowerPoint, Excels, DocX files, and PDFs. So the first step is really to parse them and to extract all the content within it in a sort of structured way. Typically we use Markdown for this. We tried many, many different parsers. I think the best one that sort of does the job most reliably and most effectively and sort of respects where our data lives is Azure Document Intelligence. We have our data boundaries that is held between the Swiss data center within our known data center for the Azure environment, cloud environment that we have. Therefore we can put up to confidential data within this environment, which complies with our safety, with our classification policy and security policies. So given that, either we could use on-prem document process, or we could use a Cloud Service that’s hosted on Azure. Azure Document Intelligence seemed like the best of the class here. The other ones that we tried were dokling, and we also tried Magnum, I think it’s called. But I think overall document intelligence was the best in retaining the layout of the documents and also extracting all of the content from the document in a structured way in a mark down format. So this is the one that we decided to use. As I mentioned, obviously, the visual elements are kind of lost. But here now, lately, we started experimenting with using document intelligence to pass the content. And document intelligence naturally captures the fact that there are figures in a particular area, and it will mark down with a figure tag. Whenever that figure tag is detected, we literally carve after a figure and send it to GPT-4 or Vision Model to summarize what is being conveyed by the figure and put it inside the document itself. So we are starting to now look at Vision-enabled document intelligence to sort of capture all of the information of a given document within a markdown file as an output. So document intelligence has really been a winning factor here for us. Then comes chunking. Once you have your past documents, you need to chunk them in a meaningful way. And there are many, many, many chunking strategies out there. There is character splitting, recursive character splitting, the semantic splitting, many, many different ways to do the chunking and splitting. I think what I’ve found best especially for office related documents and policies and procedures, these are nicely structured for human eyes in a sort of report style format. There are headings and there are sections and each section contains a unit of information that is contained within that particular section. So for me, what I found best was to split or to chunk these past documents by section. So by using Markdown headers, H1, H2, H3 headers as a way to split these documents using Langchane’s Markdown header splitter as the chunking strategy here. Another thing I think was important during chunking process was to also include in the metadata of each chunk the source of the document, the URL, where the document was searched from, the title of the document, the headings as the metadata of a particular chunk, the page number the chunk came from within the source document. So those four things are included in the chunking process. Next thing that I guess a bit more advanced, RAG systems do is keyword generation because we can do the vector search and the retrieval using both the embeddings or the vector similarity, but also using keywords. So the next step that I do, or what we did for our RAG was to send our chunks to a GPT-4.0 model to generate five to six keywords that represent what is being conveyed in that particular chunk. This is also included in the metadata of a chunk. Once all of this is done, we’ve done the parsing of the document using document intelligence. We’ve done the chunking using the Markdown headers player. Chunking method, we generated some metadata, we generated some keywords for it. Then we vectorized it using a vector database. Here are some of the lessons learned. Also is the fact that if you are able to compartmentalize the vector database into different collections of different topics, that helps with retrieval as well. Putting everything up into a single bucket while it’s okay, I found the performance to be better by sort of containerizing, hey, this is– these set of documents talk about travel-related policies. These set of documents talk about maternity leave related or travel or event-related policies and things like that. So you split it up by topic. So you can retrieve chunks and documents and objects from a particular topic based on a user’s question. If the user’s question is specific to a topic, you don’t need to retrieve irrelevant chunks or you don’t need to even search for irrelevant within irrelevant chunks from other topics. You only force in a particular topic. If a user’s question involves multiple topics, then you sort of query those multiple topics and retrieve relevant chunks for it. So by compartmentalizing the collections by topic, was a helpful strategy. Another thing for your ETL pipeline that is also good to make sure, especially in a corporate setting with these source documents, these policies may regularly get updated, so how do you ensure your rag chatbot reveals the right answer all the time is to also create a monitoring of the file sources that you have. So things like tracking file modifications, deletions, and additions. And be able to handle that. So if a file is modified or added, if the file is modified, find the chunks that you have in your vector database, destroy them, and treat it as if it’s a new file. So parse it, chunk it, and add it again. If a file is new file, do the same thing. If the file has been deleted, simply to remove all the chunks from your vector database. The way we decided to go by doing that was to maintain a state file, which sort of captured the files that have been processed. The previous ETL pipeline was run and any new files since then on a nightly basis is checked. Any new files since then I then just updates the state and then you take relevant actions accordingly. One perhaps another caution point here is where does your source data live? Do you read the golden source, or the golden copy of these files, or do you create a copy of these files, and work on those copies? How do you ensure the copies remain true to the golden source? All of these things considerations must be taken care of. We used SharePoint online folders as a repository to collect the data from for our ETL pipelines. But again, here in a corporate settings, especially in a Microsoft environment, people are starting to use SharePoint Online web pages and sites to store information and departmental policies and procedures in sort of web pages. These are dot ASP pages and perhaps building a connector to sort of fetch the data off of a particular page. Things like SharePoint on-prem, things like shared drives, all of these sources are valid and should be considered for onboarding into your rack pipeline. The other thing that we did for something else that we noticed during document intelligence while it does a really, really good job, sometimes, especially for our corporate template of Word documents, Excel documents and PowerPoint documents, we noticed that when document intelligence was sort of passing them, it did not identify all the headers correctly within the document. It identified our root header, our very first title of the document as an H1 header and then sort of put everything else, all the other content within that. It did not then identify H2 headers and H3 headers. This is a problem when you’re trying to chunk because your entire document becomes one chunk. This issue was done at random and perhaps it was linked to the corporate style of the documents that we have our own templates. Doing it on general word documents that don’t comply with our corporate template, this was never an issue, was only an issue. So there’s don’t comply with our corporate template. This was never an issue, was only an issue. So there’s something to do with our corporate template. But something like this might be of useful use of to you as well, and something for you to think about. Okay, moving on from chunking an ETL pipeline to the RAG process itself. So for example, we did a meeting services related at a rat chat pod, the meeting services department in an organization that tracks, that manages meetings, travels, event organization, meeting room bookings, catering, restaurant bookings, and all of these kind of stuff for different events and the meetings that we hold within our organization. And they have lots of policies and procedures around this and guidelines for users to book some of these services or to do some of these things. So as a rat chat bot, on top of the ETL that we built was, we would take a user’s question. And then the first thing that we do is we sort of classify the user’s question as to what topics is the user asking about. So we take the user’s question, send it to an LLM and with a prompt saying, user’s question must comply with one or many of these particular topics. If it’s outside those topics, then the user’s question is out of scope for this right chat box. Simply reply back to the user saying, we do not have relevant information for that particular question, please contact the meeting services team directly with your video inquiry. So if the user asks questions like, what is today’s weather? This is outside the scope of the RAC Chatbot, and they’ll get a standard stock standard reply. But otherwise, we’ll ask the LLM to categorize users’ question into one or multiple topics that we have within our collection within our vector database. We’ll collect those topics and there’ll be a metadata field that will pass to the to our retriever so that it can retrieve from the from the right collection. The other thing that I also do with the user’s query is sort of rephrase that query. So if you have a memory built in where the user may ask, I want to– what is the seating capacity of this particular room? And in the next question, they ask, oh, does that room support hybrid meetings? The LLM needs– the RAG service needs to know what that is referring to, the word that, and the second question, what what that is referring to, the word that in the second question, what is that even referring to? So to be able to fill in that gap, it needs the history of messages that have been passed. So we need to reformulate users’ question into a meaningful way so that the LLM can then respond. So we rephrase the query to take in previous queries. In context to fill and fill up all those gaps. Then what we do is we send, we create keywords for users query as well, and then we do, we generate embeddings for users question, and then we do sort of retrieval based on the embeddings and the keywords similarities search as well. We do BM25 retrieval here. Then we pass the retreat chunks. You can select how many, plus five, plus 10, and the user’s question to the LLM and pass all of that information to the LLM to generate a coherent answer. As per the RAG things, one of the things that we do provide back to the user as a response citations, the titles, the page numbers as well. Another thing we’ve recently added to our to our rag chat bot is a re-ranker and this has been game changer. We’ve been known while the LLM does a GPD 4.0 or less the LLM specifically does a really really good job with the jumbled sort of chunks that it is sent we sent top 10 but then not necessarily in the order that you would like some of the more sometimes sometimes irrelevant chunks are above good chunks are in the middle or at the bottom what a re-ranker is does is it does a really really good job in re-priizing and reorganizing chunks. So we’ve now started doing retrieving up to 30 or 40 chunks from the from the retriever, giving it to the re-ranker to pick out the top 10 and give those top 10 in a prioritized manner to the LLM. And you get a much better higher quality sort of response on the other end as a result of it as well. One thing I would also talk about is the–\n","wordCount":"2776","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Sukhvir Notra"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/building_rag_applications/"},"publisher":{"@type":"Organization","name":"Sukhvir's blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Sukhvir's blog (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/archives/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/categories/ title=Categories><span>Categories</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent"></h1><div class=post-meta>14 min&nbsp;·&nbsp;2776 words&nbsp;·&nbsp;Sukhvir Notra</div></header><div class=post-content><p>For the last year, I&rsquo;ve spent a lot of time developing various AI use cases and applications within my organization. And I&rsquo;ve learned a lot during this process. Majority of the applications that are built are around robotic process automation, feature extraction, and rag. Talking about rag, let&rsquo;s focus on rag in this particular article. Here are some of the few lessons that I&rsquo;ve learned. Here are the few things that I found interesting and just some general comments that I&rsquo;d like to tell you all about. So firstly, something that I think we all know, something you must have already heard about but I just want to reinforce and stress that this is, I&rsquo;ve not had a unique experience here and I&rsquo;ll face the same problem that most do. 80%, I would say minimum is the amount of time you spend just cleaning up the data and make sure data is correct before you even get to any kind of AI process use case development and code development and application development. That takes a majority of time. In a RAG use case, for most organization, yes, there are multi-modal RAGs out there, there are fancy tools available that you can watch on YouTube, but for most organizations, you&rsquo;re going for traditional data set which may include things like PowerPoints, Excels, DocX files, PDFs, these are what you&rsquo;re looking for. The problem here is most of the content inside these files is generated for human beings to look at. Of course, it&rsquo;s an office environment and that will be the case. But most content that is generated for human eyes is not necessarily designed for LLM consumption. Taking PowerPoints as an example, most of the PowerPoints are not text-heavy. They&rsquo;re visuals-heavy. They&rsquo;re infographics. There are also a lot of processes, process-centered operating procedures, policies and guidelines and related documents that most organizations have, they would like to put a rag chatbot on. They contain visual elements like infographics that may contain information within that graphic that is not necessarily reflected in the text of the document. In which case when you try to feed it to something like a large language model using and sort of parsing that data set, those that information will be lost. Only the text when you try to feed it to something like a large language model using and sort of parsing that data set, those, that information will be lost, only the text will be retained. We can certainly be, we can certainly lead to inferior products on the other end. The other thing that I&rsquo;ve quite learned is that there is a lot of value in sort of communicating to the organization, not the organization, but to the business users within the organization about the non-deterministic and stochastic nature of the LLM. It&rsquo;s important for them to understand that if they ask the same question five times, they will get five different answers. Of course, we can take a lot of measures to avoid hallucinations. I&rsquo;m not talking about hallucinations as such but they would not get a deterministic answer every single time. This is kind of important especially in a chatbot sort of scenario to sort of explain to the users that this is a built-in feature of the system not a bug. Speaking of rag implementation itself I think when it comes to the ETL pipeline or the data loading pipeline for a rag use case as an example, the very first step that you do is you&rsquo;re going to have to pass those documents. Like I said, there are four primary sets of documents that most organizations will be dealing with, PowerPoint, Excels, DocX files, and PDFs. So the first step is really to parse them and to extract all the content within it in a sort of structured way. Typically we use Markdown for this. We tried many, many different parsers. I think the best one that sort of does the job most reliably and most effectively and sort of respects where our data lives is Azure Document Intelligence. We have our data boundaries that is held between the Swiss data center within our known data center for the Azure environment, cloud environment that we have. Therefore we can put up to confidential data within this environment, which complies with our safety, with our classification policy and security policies. So given that, either we could use on-prem document process, or we could use a Cloud Service that&rsquo;s hosted on Azure. Azure Document Intelligence seemed like the best of the class here. The other ones that we tried were dokling, and we also tried Magnum, I think it&rsquo;s called. But I think overall document intelligence was the best in retaining the layout of the documents and also extracting all of the content from the document in a structured way in a mark down format. So this is the one that we decided to use. As I mentioned, obviously, the visual elements are kind of lost. But here now, lately, we started experimenting with using document intelligence to pass the content. And document intelligence naturally captures the fact that there are figures in a particular area, and it will mark down with a figure tag. Whenever that figure tag is detected, we literally carve after a figure and send it to GPT-4 or Vision Model to summarize what is being conveyed by the figure and put it inside the document itself. So we are starting to now look at Vision-enabled document intelligence to sort of capture all of the information of a given document within a markdown file as an output. So document intelligence has really been a winning factor here for us. Then comes chunking. Once you have your past documents, you need to chunk them in a meaningful way. And there are many, many, many chunking strategies out there. There is character splitting, recursive character splitting, the semantic splitting, many, many different ways to do the chunking and splitting. I think what I&rsquo;ve found best especially for office related documents and policies and procedures, these are nicely structured for human eyes in a sort of report style format. There are headings and there are sections and each section contains a unit of information that is contained within that particular section. So for me, what I found best was to split or to chunk these past documents by section. So by using Markdown headers, H1, H2, H3 headers as a way to split these documents using Langchane&rsquo;s Markdown header splitter as the chunking strategy here. Another thing I think was important during chunking process was to also include in the metadata of each chunk the source of the document, the URL, where the document was searched from, the title of the document, the headings as the metadata of a particular chunk, the page number the chunk came from within the source document. So those four things are included in the chunking process. Next thing that I guess a bit more advanced, RAG systems do is keyword generation because we can do the vector search and the retrieval using both the embeddings or the vector similarity, but also using keywords. So the next step that I do, or what we did for our RAG was to send our chunks to a GPT-4.0 model to generate five to six keywords that represent what is being conveyed in that particular chunk. This is also included in the metadata of a chunk. Once all of this is done, we&rsquo;ve done the parsing of the document using document intelligence. We&rsquo;ve done the chunking using the Markdown headers player. Chunking method, we generated some metadata, we generated some keywords for it. Then we vectorized it using a vector database. Here are some of the lessons learned. Also is the fact that if you are able to compartmentalize the vector database into different collections of different topics, that helps with retrieval as well. Putting everything up into a single bucket while it&rsquo;s okay, I found the performance to be better by sort of containerizing, hey, this is&ndash; these set of documents talk about travel-related policies. These set of documents talk about maternity leave related or travel or event-related policies and things like that. So you split it up by topic. So you can retrieve chunks and documents and objects from a particular topic based on a user&rsquo;s question. If the user&rsquo;s question is specific to a topic, you don&rsquo;t need to retrieve irrelevant chunks or you don&rsquo;t need to even search for irrelevant within irrelevant chunks from other topics. You only force in a particular topic. If a user&rsquo;s question involves multiple topics, then you sort of query those multiple topics and retrieve relevant chunks for it. So by compartmentalizing the collections by topic, was a helpful strategy. Another thing for your ETL pipeline that is also good to make sure, especially in a corporate setting with these source documents, these policies may regularly get updated, so how do you ensure your rag chatbot reveals the right answer all the time is to also create a monitoring of the file sources that you have. So things like tracking file modifications, deletions, and additions. And be able to handle that. So if a file is modified or added, if the file is modified, find the chunks that you have in your vector database, destroy them, and treat it as if it&rsquo;s a new file. So parse it, chunk it, and add it again. If a file is new file, do the same thing. If the file has been deleted, simply to remove all the chunks from your vector database. The way we decided to go by doing that was to maintain a state file, which sort of captured the files that have been processed. The previous ETL pipeline was run and any new files since then on a nightly basis is checked. Any new files since then I then just updates the state and then you take relevant actions accordingly. One perhaps another caution point here is where does your source data live? Do you read the golden source, or the golden copy of these files, or do you create a copy of these files, and work on those copies? How do you ensure the copies remain true to the golden source? All of these things considerations must be taken care of. We used SharePoint online folders as a repository to collect the data from for our ETL pipelines. But again, here in a corporate settings, especially in a Microsoft environment, people are starting to use SharePoint Online web pages and sites to store information and departmental policies and procedures in sort of web pages. These are dot ASP pages and perhaps building a connector to sort of fetch the data off of a particular page. Things like SharePoint on-prem, things like shared drives, all of these sources are valid and should be considered for onboarding into your rack pipeline. The other thing that we did for something else that we noticed during document intelligence while it does a really, really good job, sometimes, especially for our corporate template of Word documents, Excel documents and PowerPoint documents, we noticed that when document intelligence was sort of passing them, it did not identify all the headers correctly within the document. It identified our root header, our very first title of the document as an H1 header and then sort of put everything else, all the other content within that. It did not then identify H2 headers and H3 headers. This is a problem when you&rsquo;re trying to chunk because your entire document becomes one chunk. This issue was done at random and perhaps it was linked to the corporate style of the documents that we have our own templates. Doing it on general word documents that don&rsquo;t comply with our corporate template, this was never an issue, was only an issue. So there&rsquo;s don&rsquo;t comply with our corporate template. This was never an issue, was only an issue. So there&rsquo;s something to do with our corporate template. But something like this might be of useful use of to you as well, and something for you to think about. Okay, moving on from chunking an ETL pipeline to the RAG process itself. So for example, we did a meeting services related at a rat chat pod, the meeting services department in an organization that tracks, that manages meetings, travels, event organization, meeting room bookings, catering, restaurant bookings, and all of these kind of stuff for different events and the meetings that we hold within our organization. And they have lots of policies and procedures around this and guidelines for users to book some of these services or to do some of these things. So as a rat chat bot, on top of the ETL that we built was, we would take a user&rsquo;s question. And then the first thing that we do is we sort of classify the user&rsquo;s question as to what topics is the user asking about. So we take the user&rsquo;s question, send it to an LLM and with a prompt saying, user&rsquo;s question must comply with one or many of these particular topics. If it&rsquo;s outside those topics, then the user&rsquo;s question is out of scope for this right chat box. Simply reply back to the user saying, we do not have relevant information for that particular question, please contact the meeting services team directly with your video inquiry. So if the user asks questions like, what is today&rsquo;s weather? This is outside the scope of the RAC Chatbot, and they&rsquo;ll get a standard stock standard reply. But otherwise, we&rsquo;ll ask the LLM to categorize users&rsquo; question into one or multiple topics that we have within our collection within our vector database. We&rsquo;ll collect those topics and there&rsquo;ll be a metadata field that will pass to the to our retriever so that it can retrieve from the from the right collection. The other thing that I also do with the user&rsquo;s query is sort of rephrase that query. So if you have a memory built in where the user may ask, I want to&ndash; what is the seating capacity of this particular room? And in the next question, they ask, oh, does that room support hybrid meetings? The LLM needs&ndash; the RAG service needs to know what that is referring to, the word that, and the second question, what what that is referring to, the word that in the second question, what is that even referring to? So to be able to fill in that gap, it needs the history of messages that have been passed. So we need to reformulate users&rsquo; question into a meaningful way so that the LLM can then respond. So we rephrase the query to take in previous queries. In context to fill and fill up all those gaps. Then what we do is we send, we create keywords for users query as well, and then we do, we generate embeddings for users question, and then we do sort of retrieval based on the embeddings and the keywords similarities search as well. We do BM25 retrieval here. Then we pass the retreat chunks. You can select how many, plus five, plus 10, and the user&rsquo;s question to the LLM and pass all of that information to the LLM to generate a coherent answer. As per the RAG things, one of the things that we do provide back to the user as a response citations, the titles, the page numbers as well. Another thing we&rsquo;ve recently added to our to our rag chat bot is a re-ranker and this has been game changer. We&rsquo;ve been known while the LLM does a GPD 4.0 or less the LLM specifically does a really really good job with the jumbled sort of chunks that it is sent we sent top 10 but then not necessarily in the order that you would like some of the more sometimes sometimes irrelevant chunks are above good chunks are in the middle or at the bottom what a re-ranker is does is it does a really really good job in re-priizing and reorganizing chunks. So we&rsquo;ve now started doing retrieving up to 30 or 40 chunks from the from the retriever, giving it to the re-ranker to pick out the top 10 and give those top 10 in a prioritized manner to the LLM. And you get a much better higher quality sort of response on the other end as a result of it as well. One thing I would also talk about is the&ndash;</p></div><footer class=post-footer><ul class=post-tags></ul></footer><script src=https://giscus.app/client.js data-repo=sukhvir-notra/sukhvir-notra.github.io data-repo-id=R_kgDOOU5LDg data-category=General data-category-id=DIC_kwDOOU5LDs4Co1dN data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Sukhvir's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>