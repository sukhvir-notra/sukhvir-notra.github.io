[{"content":"Hey there, fellow tech enthusiasts! If you\u0026rsquo;re anything like me, you\u0026rsquo;ve probably spent the last year watching AI sweep through our industry like a California gold rush. And who can blame us? The possibilities are absolutely mind-blowing!\nThe Great AI Experimentation Phase Let me tell you about what\u0026rsquo;s happening in my organization—we\u0026rsquo;ve gone all in on AI.\nWe\u0026rsquo;ve collected close to 100 use cases organically from across teams. People are practically bursting with ideas:\nRobotic process automation (because who likes repetitive tasks? Not me!) RAG (Retrieval Augmented Generation) chatbots galore Text summarization (for those emails that could have been a text message) Image generation (our design team is both terrified and amazed) And it\u0026rsquo;s FANTASTIC to see this enthusiasm!\nFramework Fever: The Great Experimentation The experimentation phase is where things get really interesting. Different teams are testing different RAG frameworks:\nSome are going with the Langchain ecosystem (Langflow, Langraph) Others are experimenting with Langsmith A few teams are testing RAGflow And naturally, some units are building their own custom frameworks (because why use something off-the-shelf when you can spend months building your own, am I right?) # Typical RAG implementation these days from langchain import ChatOpenAI, PromptTemplate from langchain.chains import create_retrieval_chain from langchain.memory import ConversationBufferMemory # Maybe we should be adding some security checks here? retriever = vector_db.as_retriever() # But we\u0026#39;re too excited to slow down! This wild experimentation is exactly what should be happening in these early days of adoption. Eventually, we\u0026rsquo;ll converge on a more unified platform strategy. But for now? Let the thousand flowers bloom!\nThe Security Gap: Where I Start Sweating Here\u0026rsquo;s where I need to put on my cybersecurity hat (it\u0026rsquo;s white, naturally). This unchecked experimentation, while exciting and necessary, has created a concerning security gap.\nLike what exactly?\nSecurity teams can\u0026rsquo;t keep pace with all these diverse technologies Each framework introduces its own potential vulnerabilities We\u0026rsquo;re connecting these systems to data without fully understanding the exposure Open source dependencies are being added faster than we can audit them Sound familiar? I thought so!\nSecuring the AI Gold Rush So how do we balance this innovation with security? Here are some practical approaches:\n1. Embrace Sandbox Environments Set up dedicated experimentation zones where teams can play without risking production systems. Think of it as a playground with really high walls:\n# Configuration for sandbox environments OPENAI_API_KEY = os.getenv(\u0026#34;SANDBOX_OPENAI_API_KEY\u0026#34;) MODEL_ACCESS = \u0026#34;restricted\u0026#34; DATA_ACCESS = \u0026#34;synthetic_only\u0026#34; NETWORK_ACCESS = \u0026#34;isolated\u0026#34; 2. Data Hygiene Matters Be thoughtful about what data you\u0026rsquo;re feeding these systems. I\u0026rsquo;ve seen some, uh, \u0026ldquo;creative\u0026rdquo; approaches to data acquisition\n3. Upskill Your Security Team This is crucial! Our security professionals need to understand AI systems to properly defend them. The black box needs to become at least a gray box.\nAreas for security team training:\nAI/ML fundamentals RAG architecture and vulnerabilities Prompt injection attacks Vector database security Model weight poisoning The Path Forward I\u0026rsquo;m not saying we should slow down the AI adoption train—far from it! But as we race forward, let\u0026rsquo;s make sure we\u0026rsquo;re not leaving security behind at the station.\nThe organizations that will win in the AI era aren\u0026rsquo;t just the ones moving fastest; they\u0026rsquo;re the ones moving fast while maintaining appropriate guardrails. It\u0026rsquo;s about finding that sweet spot between innovation and security.\n","permalink":"http://localhost:1313/posts/cyber_implications_of_onboarding_ai/","summary":"\u003cp\u003eHey there, fellow tech enthusiasts! If you\u0026rsquo;re anything like me, you\u0026rsquo;ve probably spent the last year watching AI sweep through our industry like a California gold rush. And who can blame us? The possibilities are absolutely mind-blowing!\u003c/p\u003e\n\u003ch2 id=\"the-great-ai-experimentation-phase\"\u003eThe Great AI Experimentation Phase\u003c/h2\u003e\n\u003cp\u003eLet me tell you about what\u0026rsquo;s happening in my organization—we\u0026rsquo;ve gone \u003cem\u003eall in\u003c/em\u003e on AI.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ve collected close to 100 use cases organically from across teams. People are practically bursting with ideas:\u003c/p\u003e","title":"⛏ The AI Gold Rush: Balancing Innovation with Security"},{"content":"Are you interested in creating an AI that can generate sonnets, tell jokes, or even help with your homework? In this blog, I’ll guide you through my experience of building GPT-2, following Andrej Karpathy’s comprehensive \u0026ldquo;Let\u0026rsquo;s build GPT\u0026rdquo; tutorial series.\nIn this article, I\u0026rsquo;ll share my notes and lessons learned as I delved into the intricacies of creating a large language model. I followed along with Karpathy\u0026rsquo;s videos (check them out here) and boy, did I learn a lot!\nFor those of you who want to peek under the hood, my code (which is essentially Karpathy\u0026rsquo;s with my modifications and a ton of comments for self-reference) is available on my GitHub. Don\u0026rsquo;t worry, I promise this journey will be more fun than watching paint dry (though, let\u0026rsquo;s be honest, sometimes that can be oddly satisfying). So, without further ado, let\u0026rsquo;s dive into the world of tokens, self-attention, and the occasional AI existential crisis. Grab your favourite caffeinated beverage, and let\u0026rsquo;s get started!\nTLDR:\nThis blog chronicles the journey of building and optimising a 124-million parameter GPT-2 model. Key steps included implementing self-attention mechanisms, adding multiple attention blocks with pre-normalization, and optimizing for computational efficiency using techniques like torch.compile and flash attention. The model was trained on a powerful GPU cluster, achieving competitive validation loss and surpassing OpenAI’s GPT-2 in accuracy on the HellaSwag benchmark. Despite not reaching GPT-3’s performance, the results highlight the effectiveness of targeted optimisations in deep learning.\nUnderstanding Self-Attention in Transformer Models Self-attention is a core concept in transformer models that enables each token in a sequence to interact with other tokens, capturing dependencies and relationships within the sequence. This interaction is governed by three key components for each token: Query, Key, and Value.\nQuery represents what a token is looking for in other tokens. Key represents what information a token has that might be of interest to other tokens. Value is the actual content that will be used for the computation. To begin, let\u0026rsquo;s set up the necessary parameters and define linear layers that will project the input tokens into queries, keys, and values. The code below defines linear layers that project the input tokens into queries, keys, and values. Each of these components is essential for the self-attention mechanism. Next, I worked with an example input tensor to see how the tokens are transformed into queries, keys, and values.\nSingle attention head\nTo compute the attention weights, I used the dot product of queries and keys (line 25). This calculation determines how much focus each token should pay to other tokens by taking the dot product between q (queries) and k (keys). The result is scaled to keep values in a manageable range.\nSince I\u0026rsquo;m dealing with a decoder block, I applied a mask to ensure that each token only attends to the tokens before it in the sequence (line 29). This masking is critical in preventing a token from \u0026ldquo;seeing\u0026rdquo; future tokens, which would disrupt the sequence generation.\nFinally, I normalized the attention weights using softmax. This process of normalizing the attention scores and multiplying them with the values completes the self-attention mechanism.\nOne key observation is that self-attention does not inherently understand the order of tokens. This lack of spatial awareness means that transformers typically incorporate positional encoding to provide this information.\nIt\u0026rsquo;s also worth noting the difference between self-attention and cross-attention:\nSelf-attention: The queries, keys, and values all come from the same source. Cross-attention: The query comes from one source, while the keys and values are derived from another source. Lessons Learned: Query-Key-Value Mechanism: Understanding the distinct roles of queries, keys, and values is crucial for grasping how attention works in transformers. Masked Attention: Implementing masking in the decoder block is essential to prevent information leakage from future tokens. No Inherent Spatial Awareness: Self-attention does not inherently understand token order; positional encodings are necessary to introduce this information. Self vs. Cross-Attention: Recognizing the distinction between self-attention and cross-attention clarifies their roles within the transformer architecture. Expanding the Transformer with Attention Blocks and Pre-Normalisation Following the tutorial, the next step involved adding multiple blocks of attention heads and feedforward layers. As these layers were stacked, the neural network deepened significantly, which introduced potential optimization challenges. To mitigate these issues, the tutorial introduced the Add and Norm technique, which is crucial for maintaining stable training in deep networks.\nA notable deviation from the original \u0026ldquo;Attention is All You Need\u0026rdquo; paper was the use of pre-normalization instead of the typical post-normalization. This approach was taken to stabilize the training process as the network depth increased.\nTo prevent overfitting, a dropout layer was added. Here’s a snippet of my code that integrates these elements:\nFeedforward Network (MLP) with Dropout\nKarpathy emphasizes pre-normalization (applying LayerNorm before the linear layers) and includes a dropout layer to prevent overfitting.\nTransformer Block with Pre-Normalization and Dropout\nThe tutorial focuses solely on self-attention and feedforward blocks, without including cross-attention blocks. This is because the task at hand—text prediction—requires only past context, making cross-attention and encoders unnecessary. Instead, the implementation used a decoder-only architecture, employing a triangular mask to ensure each token only attends to previous tokens in the sequence.\nKey architectural choices highlighted in the tutorial:\nNo Cross-Attention: Focus solely on self-attention. Pre-Normalization: Layer normalization is applied before the attention and feedforward layers. Dropout: Added to prevent overfitting. Decoder-Only Architecture: The model is designed to focus on past context using triangular masking, which is well-suited for text prediction tasks. The results obtained from this implementation were:\nTrain Loss:** 1.1325 Validation Loss:** 1.1887\nHere is some sample generated text (based on the Harry Potter books corpus data set):\nhe dripped her face to get anyone else each other. \u0026lsquo;Tharge I suppose behind it talks to find and the prefect Fleur, who want for anyone, Dumbledore\u0026rsquo;s sincent, of the marble on ghostly was wait to explain him for a restretching pain, black that the pair was squart. He wondered a continue to that he had not this attacked his like for that the memor. Harry saw Alofty Luna Jords to corridor, who had told But the parchment the window, her angless stretchy awimpage had done before he could carrier tha\nThe generated text demonstrates that the model learned to produce somewhat coherent sequences, though it still includes some nonsensical phrases, which is common at this stage of training.\nLessons Learned: Deep Network Challenges: Adding multiple attention and feedforward blocks can lead to optimization issues, making techniques like Add and Norm essential. Pre-Normalization: The use of pre-normalization, as recommended in the tutorial, proved helpful in stabilizing training in deeper models. Task-Specific Design: For text prediction, a decoder-only architecture with self-attention suffices, avoiding the complexity of cross-attention and encoders. Regularization: Incorporating dropout effectively prevents overfitting, which is crucial in a deep network with many parameters. Supersize Me: Scaling Up to 124 Million Parameters But wait, there\u0026rsquo;s more! Once you\u0026rsquo;ve got your basic model up and running, it\u0026rsquo;s time to supersize it. The next phase was to create a 124-million parameter GPT-2 model.\nAt initialization, it\u0026rsquo;s expected that all vocabulary elements have a uniform probability of being the next character. Given the GPT-2 vocabulary size of 50,257, this means the initial probability for each character is 1/50257.\nGiven that the loss function is cross-entropy (or -log loss), the expected loss at initialization should be approximately:\nTraining such a large model efficiently requires thoughtful strategies. One key approach is the weight sharing scheme, which significantly reduces the number of parameters:\nCreating efficiencies: Weight sharing scheme\nThis weight sharing not only saves a significant amount of memory but also improves computational efficiency. It ensures that the model doesn\u0026rsquo;t need to maintain separate sets of weights for embedding and output, which is particularly advantageous in large-scale models like GPT-2.\nFor training, I used lambdalabs.com to set up a cluster with 8 A100 GPUs, each with 80GB of memory. This setup allowed for efficient training of the large model, which would be nearly impossible on a standard local machine.\nGPU cluster\nAnother useful trick for interacting with the code during runtime was using: import code; code.interact(local=locals()). This allowed me to pause the execution and interact with the current state of the code, which was invaluable for debugging and tweaking the model on the fly.\nExperimenting with different types of precisions, I found that using bf16 precision drastically improved performance. The time per iteration (dt) dropped from 4000ms on a local MacBook to approximately 96ms on the Lambda Labs cluster, making training much more efficient.\nLessons Learned: Expected Initial Loss: Understanding that the initial loss for a GPT-2 model is around 10.82 helps set realistic expectations at the start of training. Weight Sharing: Implementing weight sharing is a critical technique for reducing the parameter count and improving model efficiency. Efficient Hardware Use: Leveraging powerful GPUs, such as the A100s on Lambda Labs, is essential for training large models. Precision Matters: Switching to bf16 precision significantly reduces computation time, making large-scale model training more feasible. The Need for Speed: Optimizing Your AI To push the performance further, several optimizations were implemented. First up was torch.compile(), which brought the iteration time (dt) down to approximately 60ms.\nThe efficiency gain here comes from torch.compile\u0026rsquo;s ability to reduce multiple round trips between High Bandwidth Memory (HBM) and GPU cores. By streamlining calculations within the GPU cores and minimizing the data transfers back to HBM, significant time savings were achieved.\nHowever, torch.compile was just the beginning. Flash attention proved to be even more effective, especially for handling softmax operations. Flash attention fuses all attention operations within a transformer into a single, highly efficient kernel: F.scaled_dot_product_attention(q,k,v, is_causal = True)\nAnother optimization involved using non-ugly numbers—specifically, adjusting the vocabulary size from 50,257 to 50,304, a number more amenable to power-of-2 operations. This adjustment slightly increases the tensor size, padding it with extra characters, but the resulting softmax probabilities for these padded characters are effectively ignored during computations. Despite the additional characters, this tweak boosts overall efficiency.\nThese optimizations collectively improved performance by 32x.\nFurther algorithmic improvements were based on insights from the GPT-3 paper:\nAdamW Optimizer: Betas were set to 0.9 and 0.95, with an epsilon of 1e-8. Gradient Clipping: Gradients were clipped to a norm of 1.0 to prevent large updates from bad batches. Learning Rate Scheduler: Implemented cosine decay with a warmup period. Weight Decay: Applied only to weight tensors, not biases, leveraging kernel fusion. Gradient Accumulation: Simulated a large batch size (up to 0.5 million) through gradient accumulation. To fully utilize the available hardware, Distributed Data Parallel (DDP) was introduced, spreading the workload across 8 GPUs. The training script was executed using:\ntorchrun --standalone --nproc_per_node=8 train_gpt2.py ... [other args] Within this setup, gradient synchronization was carefully managed to ensure efficiency:\nThis ensures that gradients are only synchronized during the final accumulation step, reducing overhead. It\u0026rsquo;s worth noting that this feature might be deprecated in the future, so ongoing monitoring is advised.\nAs the model scaled, so did the training dataset. The Hugging Face FineWeb-edu dataset (sample-10BT subset) was chosen for its high educational content, providing a substantial training corpus.\nFor evaluation, several strategies were implemented:\nEvaluation Frequency: An evaluation and sample generation were triggered every 100th step. Evaluation Dataset: The Hellswag dataset was used for this purpose. Learning Rate Experimentation: Unlike the standard approach, a higher learning rate was tested to observe its impact on the model\u0026rsquo;s learning. Lessons Learned Torch.compile() and Flash Attention: These optimizations are key for reducing computation time and enhancing performance. Power-of-2 Adjustments: Aligning tensor sizes to power-of-2 values can improve computational efficiency. Algorithmic Tweaks: Adopting strategies from the GPT-3 paper, such as specific optimizer settings and gradient clipping, significantly stabilizes training. Distributed Training: Utilizing multiple GPUs effectively with DDP is essential for scaling large models. Dataset Expansion: Growing the dataset and incorporating high-quality content is critical as the model size increases. Custom Evaluation Strategies: Regular evaluations and testing different learning rates provide valuable insights into model performance. Conclusion After implementing the various optimizations discussed, including advanced techniques like torch.compile, flash attention, and the use of non-ugly numbers, the 124-million parameter GPT-2 model showed significant improvements in both training efficiency and performance. As seen in the training and validation loss graph, the model reached a validation loss comparable to OpenAI\u0026rsquo;s GPT-2 implementation, indicating that the optimizations were effective in maintaining model accuracy while improving computational efficiency. Notably, our model achieved a validation loss of approximately 3.0, which aligns closely with OpenAI’s GPT-2 checkpoint.\nThe HellaSwag evaluation benchmark further highlighted the strengths of our optimized GPT-2 model. While it does not yet match the performance of OpenAI’s GPT-3 model, our implementation consistently outperformed the original GPT-2 baseline in terms of accuracy, steadily climbing to nearly 30%. This demonstrates that with targeted optimizations and careful attention to both hardware and algorithmic efficiency, it is possible to build and train large-scale models that approach the performance of industry-leading implementations. These results reinforce the importance of continuous experimentation and adaptation when working with deep learning models.\n","permalink":"http://localhost:1313/posts/creating_gpt2_my_lessons_learnt/","summary":"\u003cp\u003eAre you interested in creating an AI that can generate sonnets, tell jokes, or even help with your homework? In this blog, I’ll guide you through my experience of building GPT-2, following Andrej Karpathy’s comprehensive \u0026ldquo;Let\u0026rsquo;s build GPT\u0026rdquo; tutorial series.\u003c/p\u003e\n\u003cp\u003eIn this article, I\u0026rsquo;ll share my notes and lessons learned as I delved into the intricacies of creating a large language model. I followed along with Karpathy\u0026rsquo;s videos (check them out \u003ca href=\"https://www.youtube.com/watch?v=l8pRSuU81PU\u0026amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ\u0026amp;index=10\u0026amp;ref=sukhvir-ai.ghost.io\"\u003ehere\u003c/a\u003e) and boy, did I learn a lot!\u003c/p\u003e","title":"💡 Creating GPT2: My lessons learnt"},{"content":" Neural Networks \u0026amp; Backpropagation: My First Steps with Karpathy\u0026rsquo;s Zero to Hero Welcome to the first installment of my blog series inspired by Andrej Karpathy\u0026rsquo;s \u0026ldquo;Zero to Hero\u0026rdquo; lecture series on AI and Machine Learning. Buckle up, because I\u0026rsquo;m diving headfirst into the magical world of neural networks and backpropagation!\nMy Jupyter notebook for this post: here\nA Calculus Refresher: Derivatives, Slopes, and Other Fun Stuff Karpathy kicks off the lecture with a crash course in basic calculus, focusing specifically on derivatives. Now, if you\u0026rsquo;re like me and calculus brings back memories of late-night study sessions and too much caffeine, fear not! This foundational knowledge is essential for understanding how neural networks (NNs) function.\nEssentially, I\u0026rsquo;m trying to wrap my head around how changing weights and biases—the key parameters of NNs—affect the output. In other words, derivatives help me figure out how much each weight in the network impacts the final output.\nKey takeaways:\nDerivatives and Impact: The derivative of the output with respect to each leaf node in the network shows me how much influence those nodes have on the overall output. This is crucial for tuning the NN effectively. Backpropagation Visualized: Karpathy walks through a detailed example of backpropagation using a pseudo neural network. This step-by-step process helps me visualize how backpropagation works and how chain rule derivatives come into play. Activation Functions Galore: I also touched on activation functions like tanh and sigmoid, which are the magic sauce that helps NNs make decisions. Automating Backpropagation: The lecture discusses how to algorithmically automate backpropagation and the importance of topological sort to ensure that I don\u0026rsquo;t backpropagate until the forward pass is complete. Level of Detail: Depending on my masochistic tendencies (or dedication to understanding every little detail), I can either implement the tanh function directly or break it down into its individual components. PyTorch to the Rescue: PyTorch offers some handy abstractions to make life easier. For instance, its tensors are float32 by default, but I can cast them to .double() for float64 precision, matching Python\u0026rsquo;s default floating-point precision. Also, I need to enable gradient calculation explicitly for leaf nodes (requires_grad=True), as they don\u0026rsquo;t do it by default for efficiency reasons. Here\u0026rsquo;s a snippet showing this in action:\nimport torch \u0026#39;\u0026#39;\u0026#39; Casting the tensors as doubles and enabling gradient calculation \u0026#39;\u0026#39;\u0026#39; x1 = torch.Tensor([2.0]).double() ; x1.requires_grad = True x2 = torch.Tensor([0.0]).double() ; x2.requires_grad = True w1 = torch.Tensor([-3.0]).double() ; w1.requires_grad = True w2 = torch.Tensor([1.0]).double() ; w2.requires_grad = True b = torch.Tensor([6.8813735870195432]).double() ; b.requires_grad = True # Forward pass calculation n = x1*w1 + x2*w2 + b o = torch.tanh(n) print(f\u0026#34;Output (o): {o.data.item()}\u0026#34;) # Backward pass (calculating gradients) o.backward() print(\u0026#39;--- Gradients ---\u0026#39;) print(f\u0026#39;x2 grad: {x2.grad.item()}\u0026#39;) print(f\u0026#39;w2 grad: {w2.grad.item()}\u0026#39;) print(f\u0026#39;x1 grad: {x1.grad.item()}\u0026#39;) print(f\u0026#39;w1 grad: {w1.grad.item()}\u0026#39;) print(f\u0026#39;b grad: {b.grad.item()}\u0026#39;) # Added bias gradient display Building Blocks: Neurons, Layers, and MLPs I then dove into the class definitions of a single neuron, a layer of neurons, and a Multi-Layer Perceptron (MLP). This section is all about understanding the fundamental building blocks of NNs and how they interconnect and work together.\nMain takeaways: Understanding the components is one thing; training them is another!\nThe Learning Rate Dilemma: One of the critical aspects of training NNs is choosing the appropriate learning rate. It\u0026rsquo;s a bit of a Goldilocks problem: too large a step size and I might overshoot the optimal minimum loss; too small and training will be painfully slow (or get stuck in local minima). Pro Tip - Zeroing the Grads: A key step before backpropagation in each training iteration in PyTorch is to zero out the gradients from the previous step (p.grad = 0.0). If I forget this, PyTorch accumulates gradients across iterations, leading to incorrect updates and a lot of head-scratching. Here\u0026rsquo;s a basic training loop structure illustrating this:\n# Assuming \u0026#39;n\u0026#39; is our MLP model, \u0026#39;xs\u0026#39; are inputs, \u0026#39;ys\u0026#39; are target outputs # And we have defined a loss function (e.g., Mean Squared Error implicitly below) learning_rate = 0.05 epochs = 20 for k in range(epochs): # Forward pass: Get predictions for all inputs ypred = [n(x) for x in xs] # Calculate loss (sum of squared errors example) loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred)) # Backward pass # \u0026gt;\u0026gt;\u0026gt; Crucial Step: Zero the gradients before calculating new ones \u0026lt;\u0026lt;\u0026lt; for p in n.parameters(): p.grad = 0.0 loss.backward() # Calculate gradients for this batch # Update weights and biases with torch.no_grad(): # Temporarily disable gradient tracking for updates for p in n.parameters(): p.data += -learning_rate * p.grad # Gradient descent step print(f\u0026#34;Epoch {k}, Loss: {loss.item()}\u0026#34;) # Use .item() to get Python number Summing It Up In summary, this first dive covered how neural networks are structured, how to calculate their loss (how wrong they are), and how to use backpropagation (leveraging the power of derivatives via the chain rule, implemented as gradient descent) to minimize this loss and make the network learn. I explored the tanh activation function, built the fundamental components of a neural network from scratch, and got my hands dirty with PyTorch basics.\nStay tuned for more posts in this series as I continue my journey from zero to hero in the world of AI and ML! And remember, in the immortal words of Karpathy (and perhaps a few stressed-out students), \u0026ldquo;Happy learning, and may your gradients always descend!\u0026rdquo;\n","permalink":"http://localhost:1313/posts/nn_and_backpropagation/","summary":"\u003chr\u003e\n\u003ch1 id=\"neural-networks--backpropagation-my-first-steps-with-karpathys-zero-to-hero\"\u003eNeural Networks \u0026amp; Backpropagation: My First Steps with Karpathy\u0026rsquo;s Zero to Hero\u003c/h1\u003e\n\u003cp\u003eWelcome to the first installment of my blog series inspired by Andrej Karpathy\u0026rsquo;s \u0026ldquo;Zero to Hero\u0026rdquo; lecture series on AI and Machine Learning. Buckle up, because I\u0026rsquo;m diving headfirst into the magical world of neural networks and backpropagation!\u003c/p\u003e\n\u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n      \u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"allowfullscreen\" loading=\"eager\" referrerpolicy=\"strict-origin-when-cross-origin\" src=\"https://www.youtube.com/embed/VMj-3S1tku0?autoplay=0\u0026amp;controls=1\u0026amp;end=0\u0026amp;loop=0\u0026amp;mute=0\u0026amp;start=0\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" title=\"YouTube video\"\u003e\u003c/iframe\u003e\n    \u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003eMy Jupyter notebook for this post:\u003c/strong\u003e \u003ca href=\"https://github.com/sukhvir-notra/AI-Learning-Zero-to-Hero?ref=sukhvir-ai.ghost.io\"\u003ehere\u003c/a\u003e\u003c/p\u003e","title":"🧠 Neural Networks and Backpropagation"},{"content":"Welcome to my blog! Consider this the launchpad for an exploration into some truly fascinating fields. Buckle up, because we\u0026rsquo;re about to dive headfirst into the dynamic worlds of cybersecurity, cyber operations, artificial intelligence, and honestly, probably some other interesting tech tangents that catch my eye along the way.\nThe best part? I’m inviting you to join the adventure. Let’s learn together – and maybe try to keep the sentient AI and the critical system breaches to a minimum, shall we? 😉\nWhy These Topics? You might be wondering why this particular mix. For me, these fields represent an incredible intersection of complex challenges, critical thinking, logic, and technologies that are rapidly shaping our future. It often feels like trying to keep up with a viral cat video – things move that fast! – and diving deep feels both exciting and essential. Plus, learning is always better when shared, right?\nWhat to Expect Here On this blog, you\u0026rsquo;ll get a candid look at my learning process across these areas – the breakthroughs, the roadblocks, and everything in between. Expect things like:\nDeep Dives: We\u0026rsquo;ll venture down various rabbit holes together – exploring security frameworks, operational tactics, AI concepts like Retrieval-Augmented Generation (RAG), and whatever else sparks curiosity. Projects \u0026amp; Experiments: Follow along as I tinker, build, (probably) break, and eventually (hopefully) fix things. I plan to share code snippets, lessons learned (especially from mistakes!), and those satisfying \u0026ldquo;aha!\u0026rdquo; moments. News \u0026amp; Perspectives: I’ll do my best to share interesting developments, news, and perspectives from the cyber and AI worlds, and perhaps other tech frontiers. These fields are wild rides, and it helps to stay informed together. A Few Ground Rules To make this a great space for everyone:\nNo Judgement Zone: Whether you’re just dipping your toes into these topics or you\u0026rsquo;re a seasoned pro, you\u0026rsquo;re absolutely welcome here. Let’s support each other and maybe share a laugh over our inevitable tech mishaps. Questions Welcome: Got a burning question about something we discuss? Ask away! I might not always have the answer instantly (that\u0026rsquo;s part of the learning!), but we can certainly explore it together. Stay Curious: These fields are vast and constantly evolving. Let’s approach them with curiosity, keep learning, keep questioning, and see what we can discover or even build along the way. Final Thoughts So, grab your beverage of choice (virtual popcorn still works!) and settle in. I\u0026rsquo;m genuinely excited to share this learning adventure with you as we navigate the fascinating landscapes of cyber security, operations, and AI. Let\u0026rsquo;s explore, question, and figure things out together.\nThanks for stopping by!\nCheers,\nSukhvir\n","permalink":"http://localhost:1313/posts/welcome/","summary":"\u003cp\u003eWelcome to my blog! Consider this the launchpad for an exploration into some truly fascinating fields. Buckle up, because we\u0026rsquo;re about to dive headfirst into the dynamic worlds of \u003cstrong\u003ecybersecurity, cyber operations, artificial intelligence\u003c/strong\u003e, and honestly, probably some other interesting tech tangents that catch my eye along the way.\u003c/p\u003e\n\u003cp\u003eThe best part? I’m inviting you to join the adventure. Let’s learn together – and maybe try to keep the sentient AI \u003cem\u003eand\u003c/em\u003e the critical system breaches to a minimum, shall we? 😉\u003c/p\u003e","title":"Welcome 👋"},{"content":"For the last year, I\u0026rsquo;ve spent a lot of time developing various AI use cases and applications within my organization. And I\u0026rsquo;ve learned a lot during this process. Majority of the applications that are built are around robotic process automation, feature extraction, and rag. Talking about rag, let\u0026rsquo;s focus on rag in this particular article. Here are some of the few lessons that I\u0026rsquo;ve learned. Here are the few things that I found interesting and just some general comments that I\u0026rsquo;d like to tell you all about. So firstly, something that I think we all know, something you must have already heard about but I just want to reinforce and stress that this is, I\u0026rsquo;ve not had a unique experience here and I\u0026rsquo;ll face the same problem that most do. 80%, I would say minimum is the amount of time you spend just cleaning up the data and make sure data is correct before you even get to any kind of AI process use case development and code development and application development. That takes a majority of time. In a RAG use case, for most organization, yes, there are multi-modal RAGs out there, there are fancy tools available that you can watch on YouTube, but for most organizations, you\u0026rsquo;re going for traditional data set which may include things like PowerPoints, Excels, DocX files, PDFs, these are what you\u0026rsquo;re looking for. The problem here is most of the content inside these files is generated for human beings to look at. Of course, it\u0026rsquo;s an office environment and that will be the case. But most content that is generated for human eyes is not necessarily designed for LLM consumption. Taking PowerPoints as an example, most of the PowerPoints are not text-heavy. They\u0026rsquo;re visuals-heavy. They\u0026rsquo;re infographics. There are also a lot of processes, process-centered operating procedures, policies and guidelines and related documents that most organizations have, they would like to put a rag chatbot on. They contain visual elements like infographics that may contain information within that graphic that is not necessarily reflected in the text of the document. In which case when you try to feed it to something like a large language model using and sort of parsing that data set, those that information will be lost. Only the text when you try to feed it to something like a large language model using and sort of parsing that data set, those, that information will be lost, only the text will be retained. We can certainly be, we can certainly lead to inferior products on the other end. The other thing that I\u0026rsquo;ve quite learned is that there is a lot of value in sort of communicating to the organization, not the organization, but to the business users within the organization about the non-deterministic and stochastic nature of the LLM. It\u0026rsquo;s important for them to understand that if they ask the same question five times, they will get five different answers. Of course, we can take a lot of measures to avoid hallucinations. I\u0026rsquo;m not talking about hallucinations as such but they would not get a deterministic answer every single time. This is kind of important especially in a chatbot sort of scenario to sort of explain to the users that this is a built-in feature of the system not a bug. Speaking of rag implementation itself I think when it comes to the ETL pipeline or the data loading pipeline for a rag use case as an example, the very first step that you do is you\u0026rsquo;re going to have to pass those documents. Like I said, there are four primary sets of documents that most organizations will be dealing with, PowerPoint, Excels, DocX files, and PDFs. So the first step is really to parse them and to extract all the content within it in a sort of structured way. Typically we use Markdown for this. We tried many, many different parsers. I think the best one that sort of does the job most reliably and most effectively and sort of respects where our data lives is Azure Document Intelligence. We have our data boundaries that is held between the Swiss data center within our known data center for the Azure environment, cloud environment that we have. Therefore we can put up to confidential data within this environment, which complies with our safety, with our classification policy and security policies. So given that, either we could use on-prem document process, or we could use a Cloud Service that\u0026rsquo;s hosted on Azure. Azure Document Intelligence seemed like the best of the class here. The other ones that we tried were dokling, and we also tried Magnum, I think it\u0026rsquo;s called. But I think overall document intelligence was the best in retaining the layout of the documents and also extracting all of the content from the document in a structured way in a mark down format. So this is the one that we decided to use. As I mentioned, obviously, the visual elements are kind of lost. But here now, lately, we started experimenting with using document intelligence to pass the content. And document intelligence naturally captures the fact that there are figures in a particular area, and it will mark down with a figure tag. Whenever that figure tag is detected, we literally carve after a figure and send it to GPT-4 or Vision Model to summarize what is being conveyed by the figure and put it inside the document itself. So we are starting to now look at Vision-enabled document intelligence to sort of capture all of the information of a given document within a markdown file as an output. So document intelligence has really been a winning factor here for us. Then comes chunking. Once you have your past documents, you need to chunk them in a meaningful way. And there are many, many, many chunking strategies out there. There is character splitting, recursive character splitting, the semantic splitting, many, many different ways to do the chunking and splitting. I think what I\u0026rsquo;ve found best especially for office related documents and policies and procedures, these are nicely structured for human eyes in a sort of report style format. There are headings and there are sections and each section contains a unit of information that is contained within that particular section. So for me, what I found best was to split or to chunk these past documents by section. So by using Markdown headers, H1, H2, H3 headers as a way to split these documents using Langchane\u0026rsquo;s Markdown header splitter as the chunking strategy here. Another thing I think was important during chunking process was to also include in the metadata of each chunk the source of the document, the URL, where the document was searched from, the title of the document, the headings as the metadata of a particular chunk, the page number the chunk came from within the source document. So those four things are included in the chunking process. Next thing that I guess a bit more advanced, RAG systems do is keyword generation because we can do the vector search and the retrieval using both the embeddings or the vector similarity, but also using keywords. So the next step that I do, or what we did for our RAG was to send our chunks to a GPT-4.0 model to generate five to six keywords that represent what is being conveyed in that particular chunk. This is also included in the metadata of a chunk. Once all of this is done, we\u0026rsquo;ve done the parsing of the document using document intelligence. We\u0026rsquo;ve done the chunking using the Markdown headers player. Chunking method, we generated some metadata, we generated some keywords for it. Then we vectorized it using a vector database. Here are some of the lessons learned. Also is the fact that if you are able to compartmentalize the vector database into different collections of different topics, that helps with retrieval as well. Putting everything up into a single bucket while it\u0026rsquo;s okay, I found the performance to be better by sort of containerizing, hey, this is\u0026ndash; these set of documents talk about travel-related policies. These set of documents talk about maternity leave related or travel or event-related policies and things like that. So you split it up by topic. So you can retrieve chunks and documents and objects from a particular topic based on a user\u0026rsquo;s question. If the user\u0026rsquo;s question is specific to a topic, you don\u0026rsquo;t need to retrieve irrelevant chunks or you don\u0026rsquo;t need to even search for irrelevant within irrelevant chunks from other topics. You only force in a particular topic. If a user\u0026rsquo;s question involves multiple topics, then you sort of query those multiple topics and retrieve relevant chunks for it. So by compartmentalizing the collections by topic, was a helpful strategy. Another thing for your ETL pipeline that is also good to make sure, especially in a corporate setting with these source documents, these policies may regularly get updated, so how do you ensure your rag chatbot reveals the right answer all the time is to also create a monitoring of the file sources that you have. So things like tracking file modifications, deletions, and additions. And be able to handle that. So if a file is modified or added, if the file is modified, find the chunks that you have in your vector database, destroy them, and treat it as if it\u0026rsquo;s a new file. So parse it, chunk it, and add it again. If a file is new file, do the same thing. If the file has been deleted, simply to remove all the chunks from your vector database. The way we decided to go by doing that was to maintain a state file, which sort of captured the files that have been processed. The previous ETL pipeline was run and any new files since then on a nightly basis is checked. Any new files since then I then just updates the state and then you take relevant actions accordingly. One perhaps another caution point here is where does your source data live? Do you read the golden source, or the golden copy of these files, or do you create a copy of these files, and work on those copies? How do you ensure the copies remain true to the golden source? All of these things considerations must be taken care of. We used SharePoint online folders as a repository to collect the data from for our ETL pipelines. But again, here in a corporate settings, especially in a Microsoft environment, people are starting to use SharePoint Online web pages and sites to store information and departmental policies and procedures in sort of web pages. These are dot ASP pages and perhaps building a connector to sort of fetch the data off of a particular page. Things like SharePoint on-prem, things like shared drives, all of these sources are valid and should be considered for onboarding into your rack pipeline. The other thing that we did for something else that we noticed during document intelligence while it does a really, really good job, sometimes, especially for our corporate template of Word documents, Excel documents and PowerPoint documents, we noticed that when document intelligence was sort of passing them, it did not identify all the headers correctly within the document. It identified our root header, our very first title of the document as an H1 header and then sort of put everything else, all the other content within that. It did not then identify H2 headers and H3 headers. This is a problem when you\u0026rsquo;re trying to chunk because your entire document becomes one chunk. This issue was done at random and perhaps it was linked to the corporate style of the documents that we have our own templates. Doing it on general word documents that don\u0026rsquo;t comply with our corporate template, this was never an issue, was only an issue. So there\u0026rsquo;s don\u0026rsquo;t comply with our corporate template. This was never an issue, was only an issue. So there\u0026rsquo;s something to do with our corporate template. But something like this might be of useful use of to you as well, and something for you to think about. Okay, moving on from chunking an ETL pipeline to the RAG process itself. So for example, we did a meeting services related at a rat chat pod, the meeting services department in an organization that tracks, that manages meetings, travels, event organization, meeting room bookings, catering, restaurant bookings, and all of these kind of stuff for different events and the meetings that we hold within our organization. And they have lots of policies and procedures around this and guidelines for users to book some of these services or to do some of these things. So as a rat chat bot, on top of the ETL that we built was, we would take a user\u0026rsquo;s question. And then the first thing that we do is we sort of classify the user\u0026rsquo;s question as to what topics is the user asking about. So we take the user\u0026rsquo;s question, send it to an LLM and with a prompt saying, user\u0026rsquo;s question must comply with one or many of these particular topics. If it\u0026rsquo;s outside those topics, then the user\u0026rsquo;s question is out of scope for this right chat box. Simply reply back to the user saying, we do not have relevant information for that particular question, please contact the meeting services team directly with your video inquiry. So if the user asks questions like, what is today\u0026rsquo;s weather? This is outside the scope of the RAC Chatbot, and they\u0026rsquo;ll get a standard stock standard reply. But otherwise, we\u0026rsquo;ll ask the LLM to categorize users\u0026rsquo; question into one or multiple topics that we have within our collection within our vector database. We\u0026rsquo;ll collect those topics and there\u0026rsquo;ll be a metadata field that will pass to the to our retriever so that it can retrieve from the from the right collection. The other thing that I also do with the user\u0026rsquo;s query is sort of rephrase that query. So if you have a memory built in where the user may ask, I want to\u0026ndash; what is the seating capacity of this particular room? And in the next question, they ask, oh, does that room support hybrid meetings? The LLM needs\u0026ndash; the RAG service needs to know what that is referring to, the word that, and the second question, what what that is referring to, the word that in the second question, what is that even referring to? So to be able to fill in that gap, it needs the history of messages that have been passed. So we need to reformulate users\u0026rsquo; question into a meaningful way so that the LLM can then respond. So we rephrase the query to take in previous queries. In context to fill and fill up all those gaps. Then what we do is we send, we create keywords for users query as well, and then we do, we generate embeddings for users question, and then we do sort of retrieval based on the embeddings and the keywords similarities search as well. We do BM25 retrieval here. Then we pass the retreat chunks. You can select how many, plus five, plus 10, and the user\u0026rsquo;s question to the LLM and pass all of that information to the LLM to generate a coherent answer. As per the RAG things, one of the things that we do provide back to the user as a response citations, the titles, the page numbers as well. Another thing we\u0026rsquo;ve recently added to our to our rag chat bot is a re-ranker and this has been game changer. We\u0026rsquo;ve been known while the LLM does a GPD 4.0 or less the LLM specifically does a really really good job with the jumbled sort of chunks that it is sent we sent top 10 but then not necessarily in the order that you would like some of the more sometimes sometimes irrelevant chunks are above good chunks are in the middle or at the bottom what a re-ranker is does is it does a really really good job in re-priizing and reorganizing chunks. So we\u0026rsquo;ve now started doing retrieving up to 30 or 40 chunks from the from the retriever, giving it to the re-ranker to pick out the top 10 and give those top 10 in a prioritized manner to the LLM. And you get a much better higher quality sort of response on the other end as a result of it as well. One thing I would also talk about is the\u0026ndash;\n","permalink":"http://localhost:1313/posts/building_rag_applications/","summary":"\u003cp\u003eFor the last year, I\u0026rsquo;ve spent a lot of time developing various AI use cases and applications within my organization. And I\u0026rsquo;ve learned a lot during this process. Majority of the applications that are built are around robotic process automation, feature extraction, and rag. Talking about rag, let\u0026rsquo;s focus on rag in this particular article. Here are some of the few lessons that I\u0026rsquo;ve learned. Here are the few things that I found interesting and just some general comments that I\u0026rsquo;d like to tell you all about. So firstly, something that I think we all know, something you must have already heard about but I just want to reinforce and stress that this is, I\u0026rsquo;ve not had a unique experience here and I\u0026rsquo;ll face the same problem that most do. 80%, I would say minimum is the amount of time you spend just cleaning up the data and make sure data is correct before you even get to any kind of AI process use case development and code development and application development. That takes a majority of time. In a RAG use case, for most organization, yes, there are multi-modal RAGs out there, there are fancy tools available that you can watch on YouTube, but for most organizations, you\u0026rsquo;re going for traditional data set which may include things like PowerPoints, Excels, DocX files, PDFs, these are what you\u0026rsquo;re looking for. The problem here is most of the content inside these files is generated for human beings to look at. Of course, it\u0026rsquo;s an office environment and that will be the case. But most content that is generated for human eyes is not necessarily designed for LLM consumption. Taking PowerPoints as an example, most of the PowerPoints are not text-heavy. They\u0026rsquo;re visuals-heavy. They\u0026rsquo;re infographics. There are also a lot of processes, process-centered operating procedures, policies and guidelines and related documents that most organizations have, they would like to put a rag chatbot on. They contain visual elements like infographics that may contain information within that graphic that is not necessarily reflected in the text of the document. In which case when you try to feed it to something like a large language model using and sort of parsing that data set, those that information will be lost. Only the text when you try to feed it to something like a large language model using and sort of parsing that data set, those, that information will be lost, only the text will be retained. We can certainly be, we can certainly lead to inferior products on the other end. The other thing that I\u0026rsquo;ve quite learned is that there is a lot of value in sort of communicating to the organization, not the organization, but to the business users within the organization about the non-deterministic and stochastic nature of the LLM. It\u0026rsquo;s important for them to understand that if they ask the same question five times, they will get five different answers. Of course, we can take a lot of measures to avoid hallucinations. I\u0026rsquo;m not talking about hallucinations as such but they would not get a deterministic answer every single time. This is kind of important especially in a chatbot sort of scenario to sort of explain to the users that this is a built-in feature of the system not a bug. Speaking of rag implementation itself I think when it comes to the ETL pipeline or the data loading pipeline for a rag use case as an example, the very first step that you do is you\u0026rsquo;re going to have to pass those documents. Like I said, there are four primary sets of documents that most organizations will be dealing with, PowerPoint, Excels, DocX files, and PDFs. So the first step is really to parse them and to extract all the content within it in a sort of structured way. Typically we use Markdown for this. We tried many, many different parsers. I think the best one that sort of does the job most reliably and most effectively and sort of respects where our data lives is Azure Document Intelligence. We have our data boundaries that is held between the Swiss data center within our known data center for the Azure environment, cloud environment that we have. Therefore we can put up to confidential data within this environment, which complies with our safety, with our classification policy and security policies. So given that, either we could use on-prem document process, or we could use a Cloud Service that\u0026rsquo;s hosted on Azure. Azure Document Intelligence seemed like the best of the class here. The other ones that we tried were dokling, and we also tried Magnum, I think it\u0026rsquo;s called. But I think overall document intelligence was the best in retaining the layout of the documents and also extracting all of the content from the document in a structured way in a mark down format. So this is the one that we decided to use. As I mentioned, obviously, the visual elements are kind of lost. But here now, lately, we started experimenting with using document intelligence to pass the content. And document intelligence naturally captures the fact that there are figures in a particular area, and it will mark down with a figure tag. Whenever that figure tag is detected, we literally carve after a figure and send it to GPT-4 or Vision Model to summarize what is being conveyed by the figure and put it inside the document itself. So we are starting to now look at Vision-enabled document intelligence to sort of capture all of the information of a given document within a markdown file as an output. So document intelligence has really been a winning factor here for us. Then comes chunking. Once you have your past documents, you need to chunk them in a meaningful way. And there are many, many, many chunking strategies out there. There is character splitting, recursive character splitting, the semantic splitting, many, many different ways to do the chunking and splitting. I think what I\u0026rsquo;ve found best especially for office related documents and policies and procedures, these are nicely structured for human eyes in a sort of report style format. There are headings and there are sections and each section contains a unit of information that is contained within that particular section. So for me, what I found best was to split or to chunk these past documents by section. So by using Markdown headers, H1, H2, H3 headers as a way to split these documents using Langchane\u0026rsquo;s Markdown header splitter as the chunking strategy here. Another thing I think was important during chunking process was to also include in the metadata of each chunk the source of the document, the URL, where the document was searched from, the title of the document, the headings as the metadata of a particular chunk, the page number the chunk came from within the source document. So those four things are included in the chunking process. Next thing that I guess a bit more advanced, RAG systems do is keyword generation because we can do the vector search and the retrieval using both the embeddings or the vector similarity, but also using keywords. So the next step that I do, or what we did for our RAG was to send our chunks to a GPT-4.0 model to generate five to six keywords that represent what is being conveyed in that particular chunk. This is also included in the metadata of a chunk. Once all of this is done, we\u0026rsquo;ve done the parsing of the document using document intelligence. We\u0026rsquo;ve done the chunking using the Markdown headers player. Chunking method, we generated some metadata, we generated some keywords for it. Then we vectorized it using a vector database. Here are some of the lessons learned. Also is the fact that if you are able to compartmentalize the vector database into different collections of different topics, that helps with retrieval as well. Putting everything up into a single bucket while it\u0026rsquo;s okay, I found the performance to be better by sort of containerizing, hey, this is\u0026ndash; these set of documents talk about travel-related policies. These set of documents talk about maternity leave related or travel or event-related policies and things like that. So you split it up by topic. So you can retrieve chunks and documents and objects from a particular topic based on a user\u0026rsquo;s question. If the user\u0026rsquo;s question is specific to a topic, you don\u0026rsquo;t need to retrieve irrelevant chunks or you don\u0026rsquo;t need to even search for irrelevant within irrelevant chunks from other topics. You only force in a particular topic. If a user\u0026rsquo;s question involves multiple topics, then you sort of query those multiple topics and retrieve relevant chunks for it. So by compartmentalizing the collections by topic, was a helpful strategy. Another thing for your ETL pipeline that is also good to make sure, especially in a corporate setting with these source documents, these policies may regularly get updated, so how do you ensure your rag chatbot reveals the right answer all the time is to also create a monitoring of the file sources that you have. So things like tracking file modifications, deletions, and additions. And be able to handle that. So if a file is modified or added, if the file is modified, find the chunks that you have in your vector database, destroy them, and treat it as if it\u0026rsquo;s a new file. So parse it, chunk it, and add it again. If a file is new file, do the same thing. If the file has been deleted, simply to remove all the chunks from your vector database. The way we decided to go by doing that was to maintain a state file, which sort of captured the files that have been processed. The previous ETL pipeline was run and any new files since then on a nightly basis is checked. Any new files since then I then just updates the state and then you take relevant actions accordingly. One perhaps another caution point here is where does your source data live? Do you read the golden source, or the golden copy of these files, or do you create a copy of these files, and work on those copies? How do you ensure the copies remain true to the golden source? All of these things considerations must be taken care of. We used SharePoint online folders as a repository to collect the data from for our ETL pipelines. But again, here in a corporate settings, especially in a Microsoft environment, people are starting to use SharePoint Online web pages and sites to store information and departmental policies and procedures in sort of web pages. These are dot ASP pages and perhaps building a connector to sort of fetch the data off of a particular page. Things like SharePoint on-prem, things like shared drives, all of these sources are valid and should be considered for onboarding into your rack pipeline. The other thing that we did for something else that we noticed during document intelligence while it does a really, really good job, sometimes, especially for our corporate template of Word documents, Excel documents and PowerPoint documents, we noticed that when document intelligence was sort of passing them, it did not identify all the headers correctly within the document. It identified our root header, our very first title of the document as an H1 header and then sort of put everything else, all the other content within that. It did not then identify H2 headers and H3 headers. This is a problem when you\u0026rsquo;re trying to chunk because your entire document becomes one chunk. This issue was done at random and perhaps it was linked to the corporate style of the documents that we have our own templates. Doing it on general word documents that don\u0026rsquo;t comply with our corporate template, this was never an issue, was only an issue. So there\u0026rsquo;s don\u0026rsquo;t comply with our corporate template. This was never an issue, was only an issue. So there\u0026rsquo;s something to do with our corporate template. But something like this might be of useful use of to you as well, and something for you to think about. Okay, moving on from chunking an ETL pipeline to the RAG process itself. So for example, we did a meeting services related at a rat chat pod, the meeting services department in an organization that tracks, that manages meetings, travels, event organization, meeting room bookings, catering, restaurant bookings, and all of these kind of stuff for different events and the meetings that we hold within our organization. And they have lots of policies and procedures around this and guidelines for users to book some of these services or to do some of these things. So as a rat chat bot, on top of the ETL that we built was, we would take a user\u0026rsquo;s question. And then the first thing that we do is we sort of classify the user\u0026rsquo;s question as to what topics is the user asking about. So we take the user\u0026rsquo;s question, send it to an LLM and with a prompt saying, user\u0026rsquo;s question must comply with one or many of these particular topics. If it\u0026rsquo;s outside those topics, then the user\u0026rsquo;s question is out of scope for this right chat box. Simply reply back to the user saying, we do not have relevant information for that particular question, please contact the meeting services team directly with your video inquiry. So if the user asks questions like, what is today\u0026rsquo;s weather? This is outside the scope of the RAC Chatbot, and they\u0026rsquo;ll get a standard stock standard reply. But otherwise, we\u0026rsquo;ll ask the LLM to categorize users\u0026rsquo; question into one or multiple topics that we have within our collection within our vector database. We\u0026rsquo;ll collect those topics and there\u0026rsquo;ll be a metadata field that will pass to the to our retriever so that it can retrieve from the from the right collection. The other thing that I also do with the user\u0026rsquo;s query is sort of rephrase that query. So if you have a memory built in where the user may ask, I want to\u0026ndash; what is the seating capacity of this particular room? And in the next question, they ask, oh, does that room support hybrid meetings? The LLM needs\u0026ndash; the RAG service needs to know what that is referring to, the word that, and the second question, what what that is referring to, the word that in the second question, what is that even referring to? So to be able to fill in that gap, it needs the history of messages that have been passed. So we need to reformulate users\u0026rsquo; question into a meaningful way so that the LLM can then respond. So we rephrase the query to take in previous queries. In context to fill and fill up all those gaps. Then what we do is we send, we create keywords for users query as well, and then we do, we generate embeddings for users question, and then we do sort of retrieval based on the embeddings and the keywords similarities search as well. We do BM25 retrieval here. Then we pass the retreat chunks. You can select how many, plus five, plus 10, and the user\u0026rsquo;s question to the LLM and pass all of that information to the LLM to generate a coherent answer. As per the RAG things, one of the things that we do provide back to the user as a response citations, the titles, the page numbers as well. Another thing we\u0026rsquo;ve recently added to our to our rag chat bot is a re-ranker and this has been game changer. We\u0026rsquo;ve been known while the LLM does a GPD 4.0 or less the LLM specifically does a really really good job with the jumbled sort of chunks that it is sent we sent top 10 but then not necessarily in the order that you would like some of the more sometimes sometimes irrelevant chunks are above good chunks are in the middle or at the bottom what a re-ranker is does is it does a really really good job in re-priizing and reorganizing chunks. So we\u0026rsquo;ve now started doing retrieving up to 30 or 40 chunks from the from the retriever, giving it to the re-ranker to pick out the top 10 and give those top 10 in a prioritized manner to the LLM. And you get a much better higher quality sort of response on the other end as a result of it as well. One thing I would also talk about is the\u0026ndash;\u003c/p\u003e","title":""},{"content":"There\u0026rsquo;s been a lot of talk around cyber security of artificial intelligence and there are many many many examples and papers online where people are discussing the the cyber security implications of generative AI and large language models. A lot of the examples that I\u0026rsquo;m seeing online are things what I would call social engineering of a large language model, where there are a bunch of different examples of how we can trick the large language models to bypass the safety guardrails and respond to us in a particular manner such that it bypasses a guardrail. So show me how to create a ransomware. Most large language models of today will sort of deny that request saying this goes against their content policies or ethical obligations or safety guidelines that they have been put in place from the reinforced human learning and feedback RLHF or RHLF sort of life cycles and pipelines during the development and sort of creation of these large language models. But a lot of the examples online sort of talk about prompt engineering and sort of prompt injection as a way of getting the large language models to bypass those safety and guardrails and sort of reveal the malicious answer. This is great. This is good to see. But this is, I don\u0026rsquo;t know where the cybersecurity implication here is. Yes, it\u0026rsquo;s a risk in the sense that the large language models cannot be relied upon to maintain their safety guidelines, unintended way by subjecting it to certain precisely constructed prompts to get it to do that. I would like some examples where how this will actually apply in an organization that can be leveraged into a full-on attack scenario. How can this actually work? Because this seems like social engineering of a large language model to get it to do something based on some of the ways you are trying to influence it to do it. Are there any attacks on a large language model itself akin to an attack on a software library or an application that you download from the internet which might have inherent vulnerability that the attackers might exploit? Are there any cybersecurity attacks or papers out there or proven demonstrations that do that against any open source or foundational large language models because that would be very very interesting where i can see sort of prompt engineering coming into the foray is if i imagine a scenario like where we have uh security teams are relying upon a large language model to monitor telemetry of an organization and when there is a anomaly in the telemetry when something which means there\u0026rsquo;s something going on in the organization that is unpredictable, unpredicted or unforeseen or something that is out of the ordinary. Perhaps the large language model has been enabled to execute certain binaries to go and prevent or sort of prevent or remediate whatever is going on in the organization. So what we could do as an attacker would be to sort of create malicious activity within the organization, knowing full well that it will be detected, but the goal is to be detected, and sort of plant or replace the binary or a script that the large language model uses to deal with that kind of problem. So let\u0026rsquo;s say you attempt to create, you download a malicious binary onto a Windows device in your organization. This will certainly trigger Windows Defender alerts to, for example, Sentinel. And let\u0026rsquo;s say you have a large language model that\u0026rsquo;s monitoring Sentinel alerts, and it has a script that goes ahead and quarantines or deletes the malicious binary from a Windows device, for example. So as an attacker, you planted this binary into the Windows device with the full intention of it being detected and an alert being generated. At the same time, you replace that deletion binary that the LLM uses to clean up with your own malicious script. But you do not have the permissions to launch that script, or if you do have permissions, you don\u0026rsquo;t have enough sufficient permissions like the admin. You\u0026rsquo;re not privileged escalated to execute with the right amount of permissions. So by forcing a binary onto a Windows device, you\u0026rsquo;ve triggered an alert. The large language model sort of looks at the alert. Large language model then triggers, as part of its automated processes, that deletion binary and deletion binary is corrupted in a malicious binary that you planted, and thereby sort of using a large language model as a way of privilege escalating and running a particular script of your design but this is no different to typical attacks llms in this particular case are just like any other applications that are being used to to sort of execute other files so you\u0026rsquo;re using one using a privileged process to launch another process, thereby making sure that the secondary process that has been launched has the privileges of the first process. But are there any other examples of using large language models or exploiting large language models that go beyond prompt engineering of those large language models. If so, please feel free to comment and let me know about it.\n","permalink":"http://localhost:1313/posts/cyber_attacks_on_llms/","summary":"\u003cp\u003eThere\u0026rsquo;s been a lot of talk around cyber security of artificial intelligence and there are many many many examples and papers online where people are discussing the the cyber security implications of generative AI and large language models. A lot of the examples that I\u0026rsquo;m seeing online are things what I would call social engineering of a large language model, where there are a bunch of different examples of how we can trick the large language models to bypass the safety guardrails and respond to us in a particular manner such that it bypasses a guardrail. So show me how to create a ransomware. Most large language models of today will sort of deny that request saying this goes against their content policies or ethical obligations or safety guidelines that they have been put in place from the reinforced human learning and feedback RLHF or RHLF sort of life cycles and pipelines during the development and sort of creation of these large language models. But a lot of the examples online sort of talk about prompt engineering and sort of prompt injection as a way of getting the large language models to bypass those safety and guardrails and sort of reveal the malicious answer. This is great. This is good to see. But this is, I don\u0026rsquo;t know where the cybersecurity implication here is. Yes, it\u0026rsquo;s a risk in the sense that the large language models cannot be relied upon to maintain their safety guidelines, unintended way by subjecting it to certain precisely constructed prompts to get it to do that. I would like some examples where how this will actually apply in an organization that can be leveraged into a full-on attack scenario. How can this actually work? Because this seems like social engineering of a large language model to get it to do something based on some of the ways you are trying to influence it to do it. Are there any attacks on a large language model itself akin to an attack on a software library or an application that you download from the internet which might have inherent vulnerability that the attackers might exploit? Are there any cybersecurity attacks or papers out there or proven demonstrations that do that against any open source or foundational large language models because that would be very very interesting where i can see sort of prompt engineering coming into the foray is if i imagine a scenario like where we have uh security teams are relying upon a large language model to monitor telemetry of an organization and when there is a anomaly in the telemetry when something which means there\u0026rsquo;s something going on in the organization that is unpredictable, unpredicted or unforeseen or something that is out of the ordinary. Perhaps the large language model has been enabled to execute certain binaries to go and prevent or sort of prevent or remediate whatever is going on in the organization. So what we could do as an attacker would be to sort of create malicious activity within the organization, knowing full well that it will be detected, but the goal is to be detected, and sort of plant or replace the binary or a script that the large language model uses to deal with that kind of problem. So let\u0026rsquo;s say you attempt to create, you download a malicious binary onto a Windows device in your organization. This will certainly trigger Windows Defender alerts to, for example, Sentinel. And let\u0026rsquo;s say you have a large language model that\u0026rsquo;s monitoring Sentinel alerts, and it has a script that goes ahead and quarantines or deletes the malicious binary from a Windows device, for example. So as an attacker, you planted this binary into the Windows device with the full intention of it being detected and an alert being generated. At the same time, you replace that deletion binary that the LLM uses to clean up with your own malicious script. But you do not have the permissions to launch that script, or if you do have permissions, you don\u0026rsquo;t have enough sufficient permissions like the admin. You\u0026rsquo;re not privileged escalated to execute with the right amount of permissions. So by forcing a binary onto a Windows device, you\u0026rsquo;ve triggered an alert. The large language model sort of looks at the alert. Large language model then triggers, as part of its automated processes, that deletion binary and deletion binary is corrupted in a malicious binary that you planted, and thereby sort of using a large language model as a way of privilege escalating and running a particular script of your design but this is no different to typical attacks llms in this particular case are just like any other applications that are being used to to sort of execute other files so you\u0026rsquo;re using one using a privileged process to launch another process, thereby making sure that the secondary process that has been launched has the privileges of the first process. But are there any other examples of using large language models or exploiting large language models that go beyond prompt engineering of those large language models. If so, please feel free to comment and let me know about it.\u003c/p\u003e","title":""}]